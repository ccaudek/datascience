[{"path":"index.html","id":"benvenuti","chapter":"Benvenuti","heading":"Benvenuti","text":"Benvenuti nella versione online di Data Science per psicologi. Viene qui presentato il materiale delle lezioni dell’insegnamento di Psicometria B000286 (.. 2021/2022) rivolto agli studenti del primo anno del Corso di Laurea Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze. Lo scopo di questo insegnamento è quello di fornire agli studenti un’introduzione ’analisi dei dati psicologici. Le conoscenze/competenze che verranno sviluppate questo insegnamento sono dunque quelle della Data Science applicata alla psicologia, ovvero, un insieme di conoscenze/competenze che si pongono ’intersezione tra psicologia, statistica e informatica.","code":""},{"path":"index.html","id":"la-psicologia-e-la-data-science","chapter":"Benvenuti","heading":"La psicologia e la Data Science","text":"Sembra sensato spendere due parole su una domanda che è importante per gli studenti: perché dobbiamo perdere tanto tempo studiare queste cose quando realtà quello che ci interessa è tutt’altro? Questa è una bella domanda. C’è una ragione molto semplice che dovrebbe farci capire perché la Data Science sia così importante per la psicologia. Infatti, ben pensarci, la psicologia è una disciplina intrinsecamente statistica, se per statistica intendiamo quella disciplina che studia la variazione delle caratteristiche degli individui nella popolazione. La psicologia studia gli individui ed è proprio la variabilità inter- e intra-individuale ciò che vogliamo descrivere e, certi casi, predire. questo senso, la psicologia è molto diversa dall’ingegneria, per esempio. Le proprietà di un determinato ponte sotto certe condizioni, ad esempio, sono molto simili quelle di un altro ponte, sotto le medesime condizioni. Quindi, per un ingegnere la statistica è poco importante: le proprietà dei materiali sono unicamente dipendenti dalla loro composizione e restano costanti. Ma lo stesso non può dirsi degli individui: ogni individuo è unico e cambia nel tempo. E le variazioni tra gli individui, e di un individuo nel tempo, sono l’oggetto di studio proprio della psicologia: è dunque chiaro che problemi che la psicologia si pone sono molto diversi da quelli affrontati, per esempio, dagli ingegneri. Questa è la ragione per cui abbiamo tanto bisogno della Data Science psicologia: perché la Data Science ci consente di descrivere la variazione e il cambiamento. E queste sono appunto le caratteristiche di base dei fenomeni psicologici.Sono sicuro che, leggendo queste righe, molti studenti sarà venuta mente la seguente domanda: perché non chiediamo qualche esperto di fare il “lavoro sporco” (ovvero le analisi statistiche) per noi, mentre noi (gli psicologi) ci occupiamo solo di ciò che ci interessa, ovvero dei problemi psicologici slegati dai dettagli “tecnici” della Data Science? La risposta questa domanda è che non è possibile progettare uno studio psicologico sensato senza avere almeno una comprensione rudimentale della Data Science. Le tematiche della Data Science non possono essere ignorate né dai ricercatori psicologia né da coloro che svolgono la professione di psicologo al di fuori dell’Università. Infatti, anche professionisti al di fuori dall’università non possono fare meno di leggere la letteratura psicologica più recente: il continuo aggiornamento delle conoscenze è infatti richiesto dalla deontologia della professione. Ma per potere fare questo è necessario conoscere un bel po’ di Data Science! Basta aprire caso una rivista specialistica di psicologia per rendersi conto di quanto ciò sia vero: gli articoli che riportano risultati delle ricerche psicologiche sono zeppi di analisi statistiche e di modelli formali. E la comprensione della letteratura psicologica rappresenta un requisito minimo nel bagaglio professionale dello psicologo.Le considerazioni precedenti cercano di chiarire il seguente punto: la Data Science non è qualcosa da studiare malincuore, un singolo insegnamento universitario, per poi poterla tranquillamente dimenticare. Nel bene e nel male, gli psicologi usano gli strumenti della Data Science tantissimi ambiti della loro attività professionale: particolare quando costruiscono, somministrano e interpretano test psicometrici. È dunque chiaro che possedere delle solide basi di Data Science è un tassello imprescindibile del bagaglio professionale dello psicologo. questo insegnamento verrano trattati temi base della Data Science e verrà adottato un punto di vista bayesiano, che corrisponde ’approccio più recente e sempre più diffuso psicologia.","code":""},{"path":"index.html","id":"come-studiare","chapter":"Benvenuti","heading":"Come studiare","text":"Il giusto metodo di studio per prepararsi ’esame di Psicometria è quello di seguire attivamente le lezioni, assimilare concetti via via che essi vengono presentati e verificare autonomia le procedure presentate lezione. Incoraggio gli studenti farmi domande per chiarire ciò che non è stato capito appieno. Incoraggio gli studenti utilizzare forum attivi su Moodle e, soprattutto, svolgere gli esercizi proposti su Moodle. problemi forniti su Moodle rappresentano il livello di difficoltà richiesto per superare l’esame e consentono allo studente di comprendere se le competenze sviluppate fino quel punto sono sufficienti rispetto alle richieste dell’esame.La prima fase dello studio, che è sicuramente individuale, è quella cui è necessario acquisire le conoscenze teoriche relative ai problemi che saranno presentati ’esame. La seconda fase di studio, che può essere facilitata da scambi con altri e da incontri di gruppo, porta ad acquisire la capacità di applicare le conoscenze: è necessario capire come usare un software (\\(\\textsf{R}\\)) per applicare concetti statistici alla specifica situazione del problema che si vuole risolvere. Le due fasi non sono però separate: il saper fare molto spesso ci aiuta capire meglio.\nCorrado Caudek\nMarzo 2022\n","code":""},{"path":"index.html","id":"license","chapter":"Benvenuti","heading":"License","text":"online version book licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.code public domain, licensed Creative Commons CC0 1.0 Universal (CC0 1.0).","code":""},{"path":"ch-key-notions.html","id":"ch-key-notions","chapter":"Capitolo 1 Concetti chiave","heading":"Capitolo 1 Concetti chiave","text":"La data science si pone ’intersezione tra statistica e informatica. La statistica è un insieme di metodi ugilizzati per estrarre informazioni dai dati; l’informatica implementa tali procedure un software. questo Capitolo vengono introdotti concetti fondamentali.","code":""},{"path":"ch-key-notions.html","id":"popolazioni-e-campioni","chapter":"Capitolo 1 Concetti chiave","heading":"1.1 Popolazioni e campioni","text":"Popolazione. L’analisi dei dati inizia con l’individuazione delle unità portatrici di informazioni circa il fenomeno di interesse. Si dice popolazione (o universo) l’insieme \\(\\Omega\\) delle entità capaci di fornire informazioni sul fenomeno oggetto dell’indagine statistica. Possiamo scrivere \\(\\Omega = \\{\\omega_i\\}_{=1, \\dots, n}= \\{\\omega_1, \\omega_2, \\dots, \\omega_n\\}\\), oppure \\(\\Omega = \\{\\omega_1, \\omega_2, \\dots \\}\\) nel caso di popolazioni finite o infinite, rispettivamente.L’obiettivo principale della ricerca psicologica è conoscere gli esiti psicologici e loro fattori trainanti nella popolazione. Questo è l’obiettivo delle sperimentazioni psicologiche e della maggior parte degli studi osservazionali psicologia. È quindi necessario essere molto chiari sulla popolazione cui si applicano risultati della ricerca. La popolazione può essere ben definita, ad esempio, tutte le persone che si trovavano nella città di Hiroshima al momento del bombardamento atomico e sono sopravvissute per un anno, o può essere ipotetica, ad esempio, tutte le persone depresse che hanno subito o saranno sottoposte ad un intervento psicologico. Il ricercatore deve sempre essere grado di determinare se un soggetto appartiene alla popolazione oggetto di interesse.Una sotto-popolazione è una popolazione che soddisfa proprietà ben definite. Ad esempio, potremmo essere interessati alla sotto-popolazione di uomini di età inferiore ai 20 anni o alla sotto-popolazione di pazienti depressi sottoposti ad uno specifico intervento psicologico. Molte domande scientifiche riguardano le differenze tra sotto-popolazioni; ad esempio, il confronto tra un gruppo sottoposto psicoterapia e un gruppo di controllo per determinare se il trattamento è stato efficace.Campione. Gli elementi \\(\\omega_i\\) dell’insieme \\(\\Omega\\) sono detti unità statistiche. Un sottoinsieme della popolazione, ovvero un insieme di elementi \\(\\omega_i\\), viene chiamato campione. Ciascuna unità statistica \\(\\omega_i\\) (abbreviata con u.s.) è portatrice dell’informazione che verrà rilevata mediante un’operazione di misurazione.Un campione è dunque un sottoinsieme della popolazione utilizzato per conoscere tale popolazione. differenza di una sotto-popolazione definita base chiari criteri, un campione viene generalmente selezionato tramite un procedura casuale. Il campionamento casuale consente allo scienziato di trarre conclusioni sulla popolazione e, soprattutto, di quantificare l’incertezza sui risultati. campioni di un sondaggio sono esempi di campioni casuali, ma molti studi osservazionali non sono campionati casualmente. Possono essere campioni di convenienza, come coorti di studenti un unico istituto, che consistono di tutti gli studenti sottoposti ad un certo intervento psicologico quell’istituto. Indipendentemente da come vengono ottenuti campioni, il loro uso al fine di conoscere una popolazione target significa che problemi di rappresentatività sono inevitabili e devono essere affrontati.","code":""},{"path":"ch-key-notions.html","id":"variabili-e-costanti","chapter":"Capitolo 1 Concetti chiave","heading":"1.2 Variabili e costanti","text":"Una variabile è qualsiasi proprietà o descrittore che può assumere più valori (numerici o categoriali). Una variabile può essere pensata come una domanda cui il valore è la risposta. Ad esempio, “Quanti anni ha questo partecipante?” “38 anni”. Qui, “età” è la variabile e “38” è il suo valore. La probabilità che la variabile \\(X\\) assuma valore \\(x\\) si scrive \\(P(X = x)\\). Questo è spesso abbreviato \\(P(x)\\). Possiamo anche esaminare la probabilità di più valori contemporaneamente; per esempio, la probabilità che \\(X = x\\) e \\(Y = y\\) è scritta \\(P(X = x, Y = y)\\) o \\(P(x, y)\\). Si noti che \\(P(X = 38)\\) è interpretato come la probabilità che un individuo selezionato casualmente dalla popolazione abbia 38 anni. Il termine “variabile” si contrappone al termine “costante” che descrive una proprietà invariante di tutte le unità statistiche.Si dice modalità ciascuna delle varianti con cui una variabile statistica può presentarsi. Definiamo insieme delle modalità di una variabile statistica l’insieme \\(M\\) di tutte le possibili espressioni con cui la variabile può manifestarsi. Le modalità osservate e facenti parte del campione si chiamano dati (si veda la Tabella 1.1).Esempio 1.1  Supponiamo che il fenomeno studiato sia l’intelligenza. uno studio, la popolazione potrebbe corrispondere ’insieme di tutti gli italiani adulti. La variabile considerata potrebbe essere il punteggio del test standardizzato WAIS-IV. Le modalità di tale variabile potrebbero essere \\(112, 92, 121, \\dots\\). Tale variabile è di tipo quantitativo discreto.Esempio 1.2  Supponiamo che il fenomeno studiato sia il compito Stroop. La popolazione potrebbe corrispondere ’insieme dei bambini dai 6 agli 8 anni. La variabile considerata potrebbe essere il reciproco dei tempi di reazione secondi. Le modalità di tale variabile potrebbero essere \\(1.93, 2.35, 1.32, 1.49, 1.62, 2.93, \\dots\\). La variabile è di tipo quantitativo continuo.Esempio 1.3  Supponiamo che il fenomeno studiato sia il disturbo di personalità. La popolazione potrebbe corrispondere ’insieme dei detenuti nelle carceri italiane. La variabile considerata potrebbe essere l’assessment del disturbo di personalità tramite interviste cliniche strutturate. Le modalità di tale variabile potrebbero essere Cluster , Cluster B, Cluster C descritti dal DSM-V. Tale variabile è di tipo qualitativo.","code":""},{"path":"ch-key-notions.html","id":"variabili-casuali","chapter":"Capitolo 1 Concetti chiave","heading":"1.2.1 Variabili casuali","text":"Il termine variabile usato nella statistica è equivalente al termine variabile casuale usato nella teoria delle probabilità. Lo studio dei risultati degli interventi psicologici è lo studio delle variabili casuali che misurano questi risultati. Una variabile casuale cattura una caratteristica specifica degli individui nella popolazione e suoi valori variano tipicamente tra gli individui. Ogni variabile casuale può assumere teoria una gamma di valori sebbene, pratica, osserviamo un valore specifico per ogni individuo. Quando faremo riferiremo alle variabili casuali considerate termini generali useremo lettere maiuscole come \\(X\\) e \\(Y\\); quando faremo riferimento ai valori che una variabile casuale assume determinate circostanze useremo lettere minuscole come \\(x\\) e \\(y\\).","code":""},{"path":"ch-key-notions.html","id":"variabili-indipendenti-e-variabili-dipendenti","chapter":"Capitolo 1 Concetti chiave","heading":"1.2.2 Variabili indipendenti e variabili dipendenti","text":"Un primo compito fondamentale qualsiasi analisi dei dati è l’identificazione delle variabili dipendenti (\\(Y\\)) e delle variabili indipendenti (\\(X\\)). Le variabili dipendenti sono anche chiamate variabili di esito o di risposta e le variabili indipendenti sono anche chiamate predittori o covariate. Ad esempio, nell’analisi di regressione, che esamineremo seguito, la domanda centrale è quella di capire come \\(Y\\) cambia al variare di \\(X\\). Più precisamente, la domanda che viene posta è: se il valore della variabile indipendente \\(X\\) cambia, qual è la conseguenza per la variabile dipendente \\(Y\\)? parole povere, le variabili indipendenti e dipendenti sono analoghe “cause” ed “effetti”, laddove le virgolette usate qui sottolineano che questa è solo un’analogia e che la determinazione delle cause può avvenire soltanto mediante l’utilizzo di un appropriato disegno sperimentale e di un’adeguata analisi statistica.Se una variabile è una variabile indipendente o dipendente dipende dalla domanda di ricerca. volte può essere difficile decidere quale variabile è dipendente e quale è indipendente, particolare quando siamo specificamente interessati ai rapporti di causa/effetto. Ad esempio, supponiamo di indagare l’associazione tra esercizio fisico e insonnia. Vi sono evidenze che l’esercizio fisico (fatto al momento giusto della giornata) può ridurre l’insonnia. Ma l’insonnia può anche ridurre la capacità di una persona di fare esercizio fisico. questo caso, dunque, non è facile capire quale sia la causa e quale l’effetto, quale sia la variabile dipendente e quale la variabile indipendente. La possibilità di identificare il ruolo delle variabili (dipendente/indipendente) dipende dalla nostra comprensione del fenomeno esame.Esempio 1.4  Uno psicologo convoca 120 studenti universitari per un test di memoria.\nPrima di iniziare l’esperimento, metà dei soggetti viene detto che si\ntratta di un compito particolarmente difficile; agli altri soggetti non\nviene data alcuna indicazione. Lo psicologo misura il punteggio nella\nprova di memoria di ciascun soggetto.questo esperimento, la variabile indipendente è l’informazione sulla difficoltà della prova. La variabile indipendente viene manipolata dallo sperimentatore assegnando soggetti (di solito maniera causale) o alla condizione (modalità) “informazione assegnata” o “informazione non data”. La\nvariabile dipendente è ciò che viene misurato nell’esperimento, ovvero\nil punteggio nella prova di memoria di ciascun soggetto.","code":""},{"path":"ch-key-notions.html","id":"la-matrice-dei-dati","chapter":"Capitolo 1 Concetti chiave","heading":"1.2.3 La matrice dei dati","text":"Le realizzazioni delle variabili esaminate una rilevazione statistica\nvengono organizzate una matrice dei dati. Le colonne della matrice\ndei dati contengono gli insiemi dei dati individuali di ciascuna\nvariabile statistica considerata. Ogni riga della matrice contiene tutte\nle informazioni relative alla stessa unità statistica. Una generica\nmatrice dei dati ha l’aspetto seguente:\\[\nD_{m,n} =\n\\begin{pmatrix}\n  \\omega_1 & a_{1}   & b_{1}   & \\cdots & x_{1} & y_{1}\\\\\n  \\omega_2 & a_{2}   & b_{2}   & \\cdots & x_{2} & y_{2}\\\\\n  \\vdots   & \\vdots  & \\vdots  & \\ddots & \\vdots & \\vdots  \\\\\n\\omega_n  & a_{n}   & b_{n}   & \\cdots & x_{n} & y_{n}\n\\end{pmatrix}\n\\]dove, nel caso presente, la prima colonna contiene il\nnome delle unità statistiche, la seconda e la terza colonna si\nriferiscono due mutabili statistiche (variabili categoriali; \\(\\) e\n\\(B\\)) e ne presentano le modalità osservate nel campione mentre le ultime\ndue colonne si riferiscono due variabili statistiche (\\(X\\) e \\(Y\\)) e ne\npresentano le modalità osservate nel campione. Generalmente, tra le\nunità statistiche \\(\\omega_i\\) non esiste un ordine progressivo; l’indice\nattribuito alle unità statistiche nella matrice dei dati si riferisce\nsemplicemente alla riga che esse occupano.","code":""},{"path":"ch-key-notions.html","id":"parametri-e-modelli","chapter":"Capitolo 1 Concetti chiave","heading":"1.3 Parametri e modelli","text":"Ogni variabile casuale ha una distribuzione che descrive la probabilità che la variabile assuma qualsiasi valore un dato intervallo.1 Senza ulteriori specificazioni, una distribuzione può fare riferimento un’intera famiglia di distribuzioni. parametri, tipicamente indicati con lettere greche come \\(\\mu\\) e \\(\\alpha\\), ci permettono di specificare di quale membro della famiglia stiamo parlando. Quindi, si può parlare di una variabile casuale con una distribuzione Normale, ma se viene specificata la media \\(\\mu\\) = 100 e la varianza \\(\\sigma^2\\) = 15, viene individuata una specifica distribuzione Normale – nell’esempio, la distribuzione del quoziente di intelligenza.metodi statistici parametrici specificano la famiglia delle distribuzioni e quindi utilizzano dati per individuare, stimando parametri, una specifica distribuzione ’interno della famiglia di distribuzioni ipotizzata. Se \\(f\\) è la PDF di una variabile casuale \\(Y\\), l’interesse può concentrarsi sulla sua media e varianza. Nell’analisi di regressione, ad esempio, cerchiamo di spiegare come parametri di \\(f\\) dipendano dalle covariate \\(X\\). Nella regressione lineare classica, assumiamo che \\(Y\\) abbia una distribuzione normale con media \\(\\mu = \\mathbb{E}(Y)\\), e stimiamo come \\(\\mathbb{E}(Y)\\) dipenda da \\(X\\). Poiché molti esiti psicologici non seguono una distribuzione normale, verranno introdotte distribuzioni più appropriate per questi risultati. metodi non parametrici, invece, non specificano una famiglia di distribuzioni per \\(f\\). queste dispense faremo riferimento metodi non parametrici quando discuteremo della statistica descrittiva.Il termine modello è onnipresente statistica e nella data science. Il modello statistico include le ipotesi e le specifiche matematiche relative alla distribuzione della variabile casuale di interesse. Il modello dipende dai dati e dalla domanda di ricerca, ma raramente è unico; nella maggior parte dei casi, esiste più di un modello che potrebbe ragionevolmente usato per affrontare la stessa domanda di ricerca e avendo disposizione dati osservati. Nella previsione delle aspettative future dei pazienti depressi che discuteremo seguito (Zetsche et al., 2019), ad esempio, la specifica del modello include l’insieme delle covariate candidate, l’espressione matematica che collega predittori con le aspettative future e qualsiasi ipotesi sulla distribuzione della variabile dipendente. La domanda di cosa costituisca un buon modello è una domanda su cui torneremo ripetutamente questo insegnamento.","code":""},{"path":"ch-key-notions.html","id":"effetto","chapter":"Capitolo 1 Concetti chiave","heading":"1.4 Effetto","text":"L’effetto è una qualche misura dei dati. Dipende dal tipo di dati e dal tipo di test statistico che si vuole utilizzare. Ad esempio, se viene lanciata una moneta 100 volte e esce testa 66 volte, l’effetto sarà 66/100. Diventa poi possibile confrontare l’effetto ottenuto con l’effetto nullo che ci si aspetterebbe da una moneta bilanciata (50/100), o con qualsiasi altro effetto che può essere scelto. La dimensione dell’effetto si riferisce alla differenza tra l’effetto misurato nei dati e l’effetto nullo (di solito un valore che ci si aspetta di ottenere base al caso soltanto).","code":""},{"path":"ch-key-notions.html","id":"stima-e-inferenza","chapter":"Capitolo 1 Concetti chiave","heading":"1.5 Stima e inferenza","text":"La stima è il processo mediante il quale il campione viene utilizzato per conoscere le proprietà di interesse della popolazione. La media campionaria è una stima naturale della media della popolazione e la mediana campionaria è una stima naturale della mediana della popolazione. Quando parliamo di stimare una proprietà della popolazione (volte indicata come parametro della popolazione) o di stimare la distribuzione di una variabile casuale, stiamo parlando dell’utilizzo dei dati osservati per conoscere le proprietà di interesse della popolazione. L’inferenza statistica è il processo mediante il quale le stime campionarie vengono utilizzate per rispondere domande di ricerca e per valutare specifiche ipotesi relative alla popolazione. Discuteremo le procedure bayesiane dell’inferenza nell’ultima parte di queste dispense.","code":""},{"path":"ch-key-notions.html","id":"metodi-e-procedure-della-psicologia","chapter":"Capitolo 1 Concetti chiave","heading":"1.6 Metodi e procedure della psicologia","text":"Un modello psicologico di un qualche aspetto del comportamento umano o della mente ha le seguenti proprietà:descrive le caratteristiche del comportamento questione,formula predizioni sulle caratteristiche future del comportamento,è sostenuto da evidenze empiriche,deve essere falsificabile (ovvero, linea di principio, deve\npotere fare delle predizioni su aspetti del fenomeno considerato che\nnon sono ancora noti e che, se venissero indagati, potrebbero\nportare rigettare il modello, se si dimostrassero incompatibili con\nesso).L’analisi dei dati valuta un modello psicologico utilizzando strumenti statistici.Questa dispensa è strutturata maniera tale da rispecchiare la suddivisione tra temi della misurazione, dell’analisi descrittiva e dell’inferenza. Nel prossimo Capitolo sarà affrontato il tema della misurazione e, nell’ultima parte della dispensa verrà discusso l’argomento più difficile, quello dell’inferenza. Prima di affrontare il secondo tema, l’analisi descrittiva dei dati, sarà necessario introdurre il linguaggio di programmazione statistica R (un’introduzione R è fornita Appendice). Inoltre, prima di potere discutere l’inferenza, dovranno essere introdotti concetti di base della teoria delle probabilità, quanto l’inferenza non è che l’applicazione della teoria delle probabilità ’analisi dei dati.","code":""},{"path":"ch-bayes-workflow.html","id":"ch-bayes-workflow","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"Capitolo 2 Credibilità, modelli e parametri","text":"L’obiettivo di questo Capitolo è di introdurre il quadro concettuale dell’analisi dei dati bayesiana.","code":""},{"path":"ch-bayes-workflow.html","id":"analisi-dei-dati-bayesiana","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.1 Analisi dei dati bayesiana","text":"L’analisi dei dati bayesiana si basa su due idee fondamentali.La prima idea è quella della riallocazione della credibilità tra le possibilità.La seconda idea è che le possibili ipotesi, cui attribuiamo diversi gradi di credibilità, possono essere espresse valori dei parametri di un modello statistico.","code":""},{"path":"ch-bayes-workflow.html","id":"prima-idea-riallocazione-della-credibilità","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.1.1 Prima idea: riallocazione della credibilità","text":"Consideriamo la prima idea. Kruschke (2014) la descrive facendo un riferimento letterario. Il detective immaginario Sherlock Holmes spesso diceva al suo compagno, il dottor Watson: “Quante volte ti ho detto che quando hai eliminato l’impossibile, tutto ciò che rimane, per quanto improbabile, deve essere la verità?” (Doyle, 1890, cap. 6). Anche se il ragionamento di Holmes o Watson o Doyle non è mai stato descritto come un’inferenza bayesiana, realtà lo è. Nei romanzi di Doyle, Sherlock Holmes ragiona nel modo seguente. Holmes inizia elencare vari sospetti di un crimine. priori, alla possibilità che il colpevole sia uno di vari sospetti viene attribuito un certo grado di credibilità. seguito, Holmes raccoglie sistematicamente le prove che escludono alcuni possibili sospetti. Se tutti possibili sospetti tranne uno possono essere eliminati, il ragionamento bayesiano di Sherlock Holmes lo porta concludere che è pienamente credibile che il sospetto rimanente sia il colpevole, anche se ’inizio questa idea poteva sembrare poco plausibile.Kruschke (2014) esprime quest’idea con la seguente figura. Supponiamo che vi siano quattro possibili ipotesi rispetto ad un fenomeno (nella figura “Possibilities”): , B, C, D. Queste ipotesi possono corrispondere, ad esempio, ai quattro possibili colpevoli di un crimine.Nella riga superiore ci sono le ipotesi priori relativamente al “colpevole”. Nella prima colonna, la nostra credibilità priori si distribuisce equamente tra quattro possibili colpevoli. Si noti che ’intera credibilità priori assegniamo il valore 1. Se ci sono quattro possibilità, ciascuna attribuiamo il valore 0.25 se esse vengono considerate equivalenti. Se dati disposizione consentono di escludere la possibilità allora, posteriori, il livello di credibilità si ridistribuisce tra le tre rimanenti possibilità come indicato nella figura (ovvero, assegniamo ciascuna di esse il valore di 1/3). Il passaggio dalla distribuzione priori quella posteriori si dice aggiornamento bayesiano. questo caso, l’aggiornamento bayesiano (ovvero, la riallocazione della credibilità alla luce dei dati disposizione) ha solo consentito di escludere un possibile colpevole. Nella seconda colonna della figura, priori si considera impossibile la possibilità , per cui la nostra credibilità di distribuisce tra le rimanenti possibilità come indicato nella figura, se le consideriamo equivalenti. Supponendo che dati consentano di escludere la possibilità B, l’aggiornamento bayesiano ci porta alla distribuzione posteriori che attribuisce una credibilità di 0.5 alle possibilità C e D. questo caso, abbiamo escluso due possibili colpevoli, ma non sappiamo decidere tra C e D. Infine, il caso più fortunato è descritto nell’ultima colonna, nella quale priori possiamo escludere e B. dati ci consentono di escludere C. Per cui, posteriori, siamo sicuri (la credibilità è 1) che il colpevole sia D. L’analisi bayesiana procede esattamente questo modo: distribuisce la nostra credibilità priori tra una serie di possibilità. questo punto si osservano dei dati. Le informazioni fornite dai dati conducono ad una diversa distribuzione della credibilità tra le possibilità. Tale aggiornamento delle credenze conduce alla distribuzione posteriori, la quale descrive il modo cui abbiamo modificato le credenze priori alla luce delle nuove informazioni fornite dai dati.","code":""},{"path":"ch-bayes-workflow.html","id":"i-dati-sono-rumorosi-e-le-inferenze-sono-probabilistiche","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.1.2 I dati sono rumorosi e le inferenze sono probabilistiche","text":"casi della figura precedente presupponevano che dati osservati avessero relazioni deterministiche con le possibili cause. Ad esempio, Sherlock Holmes potrebbe aver trovato un’impronta sulla scena del crimine e identificato la taglia e il tipo di scarpa con assoluta certezza, escludendo così completamente o implicando un particolare candidato sospetto.realtà, dati hanno solo relazioni probabilistiche con le loro cause sottostanti. Un vero investigatore potrebbe misurare attentamente l’impronta e dettagli del suo battistrada, ma queste misurazioni restringerebbero solo probabilisticamente la gamma di possibili scarpe che potrebbero aver prodotto l’impronta. Le misure non sono perfette e l’impronta è solo una rappresentazione imperfetta della scarpa che l’ha prodotta. La relazione tra la causa (cioè la scarpa) e l’effetto misurato (cioè l’impronta) viene complicata dalla presenza del “rumore della misurazione”.Nella ricerca scientifica, le misurazioni sono fortemente contaminate dagli effetti di molteplici influenze estranee, nonostante gli enormi sforzi per limitare la loro intrusione. Per esempio, uno studio sull’apprendimento della statistica da parte degli studenti di psicologia, un ricercatore potrebbe dividere gli studenti due gruppi: un gruppo sperimentale (che affianca lo studio del materiale dell’esame con una serie di esercitazioni nelle quali viene utilizzato un software per l’analisi dei dati) e un gruppo di controllo (che studia il materiale dell’esame su un testo e si limita svolgere degli esercizi scolastici carta-e-penna). Supponiamo che il ricercatore misuri la prestazione ’esame dei due gruppi. È chiaro che la prestazione ’esame per ogni singolo studente, al di là dell’effetto della manipolazione sperimentale, può variare notevolmente seconda di molte altre influenze, come la motivazione, l’ansia, la preparazione pregressa, la quantità di studio, ecc.dati risultanti, quindi, saranno estremamente rumorosi, con un’enorme variabilità ’interno di ciascun gruppo e un’enorme sovrapposizione tra gruppi. Pertanto, ci saranno molte prestazioni ’esame misurate nel gruppo sperimentale che sono più alte delle prestazioni ’esame nel gruppo di controllo e viceversa. Da queste due distribuzioni di voti ’esame, molto disperse e sovrapposte, vogliamo inferire quanta differenza c’è tra due gruppi e quanto possiamo essere certi di una tale differenza.Tutti dati scientifici hanno un certo grado di “rumore” nei loro valori. Le tecniche di analisi dei dati sono progettate per inferire le tendenze sottostanti da dati rumorosi. differenza di Sherlock Holmes, che potrebbe fare un’osservazione ed escludere completamente alcune possibili cause, nella ricerca scientifica possiamo raccogliere dati e modificare solo un modo incrementale la credibilità di alcune possibili tendenze. Vedremo molti esempi realistici di questo processo questo insegnamento. Il bello dell’analisi bayesiana è che, alla luce delle informazioni fornite dai dati, la teoria delle probabilità ci consente di riallocare la credibilità un modo non arbitrario e automatico.","code":""},{"path":"ch-bayes-workflow.html","id":"seconda-idea-le-possibilità-sono-valori-di-parametri-in-un-modello-statistico","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.1.3 Seconda idea: le possibilità sono valori di parametri in un modello statistico","text":"BDI-II prima e dopo un intervento psicologico.Nel precedente ragionamento “bayesiano” di Sherlock Holmes, le possibilità corrispondevano alle quattro modalità di una variabile discreta: la variabile era “possibili colpevoli del crimine” e le modalità erano appunto , B, C e D.statistica, lavorare con variabili discrete è complicato. È molto più semplice, invece, svolgere l’aggiornamento bayesiano mediante gli strumenti della teoria delle probabilità, se vengono utilizzate variabili continue.tali circostanze, le “possibilità” corrispondono ai valori dei parametri un modello statistico. Possiamo chiarire questa affermazione nel modo seguente.","code":""},{"path":"ch-bayes-workflow.html","id":"flusso-di-lavoro-bayesiano","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.2 Flusso di lavoro bayesiano","text":"La moderna statistica bayesiana viene per lo più eseguita utilizzando un linguaggio di programmazione probabilistico implementato su computer. Ciò ha cambiato radicalmente il modo cui venivano eseguite le statistiche bayesiane anche fin pochi decenni fa. La complessità dei modelli che possiamo costruire è aumentata e la barriera delle competenze matematiche e computazionali che sono richieste è diminuita. Inoltre, il processo di modellazione iterativa è diventato, sotto molti aspetti, molto più facile da eseguire. Anche se formulare modelli statistici complessi è diventato più facile che mai, la statistica è un campo pieno di sottigliezze che non scompaiono magicamente utilizzando potenti metodi computazionali. Pertanto, avere una buona preparazione sugli aspetti teorici, specialmente quelli rilevanti per la pratica, è estremamente utile per applicare efficacemente metodi statistici.","code":""},{"path":"ch-bayes-workflow.html","id":"modellizzazione-bayesiana","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.3 Modellizzazione bayesiana","text":"Nell’approccio bayesiano ’inferenza statistica si prende considerazione una variabile casuale \\(Y\\) di cui si conosce la distribuzione meno di un parametro \\(\\theta\\). Secondo l’approccio bayesiano, è possibile modellare l’incertezza sul valore del parametro rappresentandolo con una variabile casuale continua \\(\\Theta\\) avente come supporto l’insieme dei valori ammissibili per il parametro cercato. La funzione di densità \\(p(\\theta)\\) prende il nome di distribuzione priori e rappresenta la sintesi delle opinioni e delle informazioni che si hanno sul parametro prima dell’osservazione dei dati. L’aggiornamento dell’incertezza su \\(\\theta\\) è determinata dal verificarsi dell’evidenza \\(y\\), ovvero dall’osservazione dei risultati di un esperimento casuale. Le informazioni provenienti dal campione osservato \\(y = (y_1, \\dots, y_n)\\) sono contenute nella funzione \\(p(y \\mid \\theta)\\), che, osservata come funzione di \\(\\theta\\) per \\(y\\), prende il nome di funzione di verosimiglianza. L’aggiornamento delle conoscenze priori incorporate nella distribuzione iniziale \\(p(\\theta)\\) seguito al verificarsi di \\(Y = y\\) (evidenza empirica) avviene attraverso il teorema di Bayes cui \\(p(\\theta \\mid y)\\) risulta proporzionale al prodotto della probabilità priori e della verosimiglianza e prende il nome di distribuzione posteriori:\\[\\begin{equation}\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta}p(y \\mid \\theta) p(\\theta) \\,\\operatorname {d}\\!\\theta} \\quad \\theta \\\\Theta.\n\\tag{2.1}\n\\end{equation}\\]Si noti che l’integrale al denominatore della (2.1) è spesso di difficile risoluzione analitica per cui l’inferenza bayesiana solitamente procede attraverso metodi di ricampionamento e metodi iterativi, quali le Catene di Markov Monte Carlo (MCMC).Martin et al. (2022) descrivono la modellazione bayesiana distinguendo tre passaggi.Dati alcuni dati e alcune ipotesi su come questi dati potrebbero essere stati generati, si progetta un modello statistico combinando e trasformando variabili casuali.Si usa il teorema di Bayes per condizionare il modello ai dati. Questo processo viene chiamato “inferenza” e come risultato si ottiene una distribuzione posteriori.Si critica il modello utilizzando criteri diversi, inclusi dati e la nostra conoscenza del dominio, per verificare se abbia senso. Poiché generale siamo incerti sul modello, volte si confrontano modelli diversi.Questi tre passaggi vengono eseguiti modo iterativo e danno luogo quello che è chiamato “flusso di lavoro bayesiano” (bayesian workflow).","code":""},{"path":"ch-bayes-workflow.html","id":"notazione","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.3.1 Notazione","text":"Per fissare la notazione, nel seguito \\(y\\) rappresenterà dati e \\(\\theta\\) rappresenterà parametri incogniti di un modello statistico. Sia \\(y\\) che \\(\\theta\\) vengono concepiti come variabili casuali. Con \\(x\\) vengono invece denotate le quantità note, come ad esempio predittori del modello lineare. Per rappresentare un modo conciso modelli probabilistici viene usata una notazione particolare. Ad esempio, invece di scrivere \\(p(\\theta) = \\mbox{Beta}(1, 1)\\) scriviamo \\(\\theta \\sim \\mbox{Beta}(1, 1)\\). Il simbolo “\\(\\sim\\)” viene spesso letto “è distribuito come”. Possiamo anche pensare che significhi che \\(\\theta\\) costituisce un campione casuale estratto dalla distribuzione Beta(1, 1). Allo stesso modo, ad esempio, la verosimiglianza del modello binomiale può essere scritta come \\(y \\sim \\text{Bin}(n, \\theta)\\).","code":""},{"path":"ch-bayes-workflow.html","id":"distribuzioni-a-priori","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.4 Distribuzioni a priori","text":"Quando adottiamo un approccio bayesiano, parametri della distribuzione di riferimento non venono considerati come delle costanti incognite ma bensì vengono trattati come variabili casuali; di conseguenza, parametri assumono una particolare distribuzione che nelle statistica bayesiana viene definita “priori”. parametri \\(\\theta\\) possono assumere delle distribuzioni priori differenti: seconda delle informazioni disponibili bisogna selezionare una distribuzione di \\(\\theta\\) modo tale che venga assegnata una probabilità maggiore quei valori del parametro che si ritengono più plausibili. Idealmente, le credenze priori che portano alla specificazione di una distribuzione priori dovrebbero essere supportate da una qualche motivazione, come ad esempio risultati di ricerche precedenti.","code":""},{"path":"ch-bayes-workflow.html","id":"tipologie-di-distribuzioni-a-priori","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.4.1 Tipologie di distribuzioni a priori","text":"Possiamo distinguere tra diverse distribuzioni priori base quanto fortemente impegnano il ricercatore ritenere come plausibile un particolare intervallo di valori dei parametri. Il caso più estremo è quello che rivela una totale assenza di conoscenze priori, il che conduce alle distribuzioni priori non informative, ovvero quelle che assegnano lo stesso livello di fiducia tutti valori dei parametri. Le distribuzioni priori informative, d’altra parte, possono essere debolmente informative o fortemente informative, seconda del modo cui lo sperimentatore distribuisce la sua fiducia nello spazio del parametro. Il caso più estremo di credenza priori è quello che assegna tutta la probabilità ad un singolo valore del parametro. La figura seguente mostra alcuni esempi di distribuzioni priori per il modello Binomiale:distribuzione non informativa: \\(\\theta_c \\sim \\mbox{Beta}(1,1)\\);distribuzione debolmente informativa: \\(\\theta_c \\sim \\mbox{Beta}(5,2)\\);distribuzione fortemente informativa: \\(\\theta_c \\sim \\mbox{Beta}(50,20)\\);valore puntuale: \\(\\theta_c \\sim \\mbox{Beta}(\\alpha, \\beta)\\) con \\(\\alpha, \\beta \\rightarrow \\infty\\) e \\(\\frac{\\alpha}{\\beta} = \\frac{5}{2}\\).\nFIGURA 2.1: Esempi di distribuzioni priori per il parametro \\(\\theta_c\\) nel Modello Binomiale.\n","code":""},{"path":"ch-bayes-workflow.html","id":"selezione-della-distribuzione-a-priori","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.4.2 Selezione della distribuzione a priori","text":"La selezione delle distribuzioni priori è stata spesso vista come una delle scelte più importanti che un ricercatore fa quando implementa un modello bayesiano quanto può avere un impatto sostanziale sui risultati finali. La soggettività delle distribuzioni priori è evidenziata dai critici come un potenziale svantaggio dei metodi bayesiani. questa critica, Schoot et al. (2021) rispondono dicendo che, al di là della scelta delle distribuzioni priori, ci sono molti elementi del processo di inferenza statistica che sono soggettivi, ovvero la scelta del modello statistico e le ipotesi sulla distribuzione degli errori. secondo luogo, Schoot et al. (2021) notano come le distribuzioni priori svolgono due importanti ruoli statistici: quello della “regolarizzazione della stima”, ovvero, il processo che porta ad indebolire l’influenza indebita di osservazioni estreme, e quello del miglioramento dell’efficienza della stima, ovvero, la facilitazione dei processi di calcolo numerico di stima della distribuzione posteriori. L’effetto della distribuzione priori sulla distribuzione posteriori verrà discusso dettaglio nel Capitolo ??.","code":""},{"path":"ch-bayes-workflow.html","id":"unapplicazione-empirica","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.4.3 Un’applicazione empirica","text":"Per introdurre la modelizzazione bayesiana useremo qui dati riportati da Zetsche et al. (2019) (si veda l’appendice ??). Tali dati corrispondono 23 “successi” 30 prove e possono dunque essere considerati la manifestazione di una variabile casuale Bernoulliana.Se non abbiamo alcuna informazione priori su \\(\\theta\\) (ovvero, la probabilità che l’aspettativa dell’umore futuro del partecipante sia distorta negativamente), potremmo pensare di usare una distribuzione priori uniforme, ovvero una Beta di parametri \\(\\alpha=1\\) e \\(\\beta=1\\). Una tale scelta, tuttavia, è sconsigliata quanto è più vantaggioso usare una distribuzione debolmente informativa, come ad esempio \\(\\mbox{Beta}(2, 2)\\), che ha come scopo la regolarizzazione, cioè quello di mantenere le inferenze un intervallo ragionevole. Qui useremo una \\(\\mbox{Beta}(2, 10)\\).\\[\np(\\theta) = \\frac{\\Gamma(12)}{\\Gamma(2)\\Gamma(10)}\\theta^{2-1} (1-\\theta)^{10-1}.\n\\]La \\(\\mbox{Beta}(2, 10)\\) esprime la credenza che \\(\\theta\\) assume valori \\(< 0.5\\), con il valore più plausibile pari circa 0.1. Questo è assolutamente implausibile per il caso dell’esempio discussione: la \\(\\mbox{Beta}(2, 10)\\) verrà usata solo per scopi didattici, ovvero, per esplorare le conseguenze di tale scelta sulla distribuzione posteriori.","code":"\nbayesrules::plot_beta(alpha = 2, beta = 10, mean = TRUE, mode = TRUE)"},{"path":"ch-bayes-workflow.html","id":"la-funzione-di-verosimiglianza","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.5 La funzione di verosimiglianza","text":"Iniziamo con una definizione.Definizione 2.1  La funzione di verosimiglianza \\(\\mathcal{L}(\\theta \\mid y) = f(y \\mid \\theta), \\theta \\\\Theta,\\) è la funzione di massa o di densità di probabilità dei dati \\(y\\) vista come una funzione del parametro sconosciuto (o dei parametri sconosciuti) \\(\\theta\\).Detto altre parole, le funzioni di verosimiglianza e di (massa o densità di) probabilità sono formalmente identiche, ma è completamente diversa la loro interpretazione. Nel caso della funzione di massa o di densità di probabilità la distribuzione del vettore casuale delle osservazioni campionarie \\(y\\) dipende dai valori assunti dal parametro (o dai parametri) \\(\\theta\\); nel caso della la funzione di verosimiglianza la credibilità assegnata ciascun possibile valore \\(\\theta\\) viene determinata avendo acquisita l’informazione campionaria \\(y\\) che rappresenta l’elemento condizionante. altri termini, la funzione di verosimiglianza descrive termini relativi il sostegno empirico che \\(\\theta \\\\Theta\\) riceve da \\(y\\). Infatti, la funzione di verosimiglianza assume forme diverse al variare di \\(y\\). Possiamo dunque pensare alla funzione di verosimiglianza come alla risposta alla seguente domanda: avendo osservato dati \\(y\\), quanto risultano (relativamente) credibili diversi valori del parametro \\(\\theta\\)? termini più formali possiamo dire: sulla base dei dati, \\(\\theta_1 \\\\Theta\\) risulta più credibile di \\(\\theta_2 \\\\Theta\\) quale indice del modello probabilistico generatore dei dati se \\(\\mathcal{L}(\\theta_1) > \\mathcal{L}(\\theta_1)\\).Notiamo un punto importante: la funzione \\(\\mathcal{L}(\\theta \\mid y)\\) non è una funzione di densità. Infatti, essa non racchiude un’area unitaria.","code":""},{"path":"ch-bayes-workflow.html","id":"notazione-1","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.5.1 Notazione","text":"Seguendo una pratica comune, questa dispensa spesso useremo la notazione \\(p(\\cdot)\\) per rappresentare due quantità differenti, ovvero la funzione di verosimiglianza e la distribuzione priori. Questo piccolo abuso di notazione riflette il seguente punto di vista: anche se la verosimiglianza non è una funzione di densità di probabilità, noi non vogliamo stressare questo aspetto, ma vogliamo piuttosto pensare alla verosimiglianza e alla distribuzione priori come due elementi che sono egualmente necessari per calcolare la distribuzione posteriori. altri termini, per così dire, questa notazione assegna lo stesso status epistemologico alle due diverse quantità che si trovano al numeratore della regola di Bayes.","code":""},{"path":"ch-bayes-workflow.html","id":"la-log-verosimiglianza","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.5.2 La log-verosimiglianza","text":"Dal punto di vista pratico risulta più conveniente utilizzare, al posto della funzione di verosimiglianza, il suo logaritmo naturale, ovvero la funzione di log-verosimiglianza:\\[\\begin{equation}\n\\ell(\\theta) = \\log \\mathcal{L}(\\theta).\n\\end{equation}\\]Poiché il logaritmo è una funzione strettamente crescente (usualmente si considera il logaritmo naturale), allora \\(\\mathcal{L}(\\theta)\\) e \\(\\ell(\\theta)\\) assumono il massimo (o punti di massimo) corrispondenza degli stessi valori di \\(\\theta\\) (per un approfondimento, si veda l’Appendice ??):\\[\n\\hat{\\theta} = \\argmax_{\\theta \\\\Theta} \\ell(\\theta) = \\argmax_{\\theta \\\\Theta} \\mathcal{L}(\\theta).\n\\]Per le proprietà del logaritmo, si ha\\[\\begin{equation}\n\\ell(\\theta) = \\log \\left( \\prod_{= 1}^n f(y \\mid \\theta) \\right) = \\sum_{= 1}^n \\log f(y \\mid \\theta).\n\\end{equation}\\]Si noti che non è necessario lavorare con logaritmi, ma è fortemente consigliato. Il motivo è che valori della verosimiglianza, cui si moltiplicano valori di probabilità molto piccoli, possono diventare estremamente piccoli – qualcosa come \\(10^{-34}\\). tali circostanze, non è sorprendente che programmi dei computer mostrino problemi di arrotondamento numerico. Le trasformazioni logaritmiche risolvono questo problema.","code":""},{"path":"ch-bayes-workflow.html","id":"unapplicazione-empirica-1","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.5.3 Un’applicazione empirica","text":"Se dati di Zetsche et al. (2019) possono essere riassunti da una proporzione allora è sensato adottare un modello probabilistico binomiale quale meccanismo generatore dei dati:\\[\\begin{equation}\ny  \\sim \\mbox{Bin}(n, \\theta),\n\\tag{2.2}\n\\end{equation}\\]laddove \\(\\theta\\) è la probabiltà che una prova Bernoulliana assuma il valore 1 e \\(n\\) corrisponde al numero di prove Bernoulliane. Questo modello assume che le prove Bernoulliane \\(y_i\\) che costituiscono il campione \\(y\\) siano tra loro indipendenti e che ciascuna abbia la stessa probabilità \\(\\theta \\[0, 1]\\) di essere un “successo” (valore 1). altre parole, il modello generatore dei dati avrà una funzione di massa di probabilità\\[\np(y \\mid \\theta)\n\\ = \\\n\\mbox{Bin}(y \\mid n, \\theta).\n\\]Nei capitoli precedenti è stato mostrato come, sulla base del modello binomiale, sia possibile assegnare una probabilità ciascun possibile valore \\(y \\\\{0, 1, \\dots, n\\}\\) assumendo noto il valore del parametro \\(\\theta\\). Ma ora abbiamo il problema inverso, ovvero quello di fare inferenza su \\(\\theta\\) alla luce dei dati campionari \\(y\\). altre parole, riteniamo di conoscere il modello probabilistico che ha generato dati, ma di tale modello non conosciamo parametri: vogliamo dunque ottenere informazioni su \\(\\theta\\) avendo osservato dati \\(y\\).Per dati di Zetsche et al. (2019) la funzione di verosimiglianza corrisponde alla funzione binomiale di parametro \\(\\theta \\[0, 1]\\) sconosciuto. Abbiamo osservato un “successo” 23 volte 30 “prove”, dunque, \\(y = 23\\) e \\(n = 30\\). La funzione di verosimiglianza diventa\\[\\begin{equation}\n\\mathcal{L}(\\theta \\mid y) = \\frac{(23 + 7)!}{23!7!} \\theta^{23} + (1-\\theta)^7.\n\\tag{2.3}\n\\end{equation}\\]Per costruire la funzione di verosimiglianza dobbiamo applicare la (2.3) tante volte, cambiando ogni volta il valore \\(\\theta\\) ma tenendo sempre costante il valore dei dati. Per esempio, se poniamo \\(\\theta = 0.1\\)\\[\n\\mathcal{L}(\\theta \\mid y) = \\frac{(23 + 7)!}{23!7!} 0.1^{23} + (1-0.1)^7\n\\]otteniamoSe poniamo \\(\\theta = 0.2\\)\\[\n\\mathcal{L}(\\theta \\mid y) = \\frac{(23 + 7)!}{23!7!} 0.2^{23} + (1-0.2)^7\n\\]otteniamoe così via. La figura 2.2 — costruita utilizzando 100 valori equispaziati \\(\\theta \\[0, 1]\\) — fornisce una rappresentazione grafica della funzione di verosimiglianza.\nFIGURA 2.2: Funzione di verosimiglianza nel caso di 23 successi 30 prove.\nCome possiamo interpretare la curva che abbiamo ottenuto? Per alcuni valori \\(\\theta\\) la funzione di verosimiglianza assume valori piccoli; per altri valori \\(\\theta\\) la funzione di verosimiglianza assume valori più grandi. Questi ultimi sono valori di \\(\\theta\\) più credibili e il valore 23/30 (la moda della funzione di verosimiglianza) è il valore più credibile di tutti.","code":"\ndbinom(23, 30, 0.1)\n#> [1] 9.737168e-18\ndbinom(23, 30, 0.2)\n#> [1] 3.581417e-11\nn <- 30\ny <- 23\ntheta <- seq(0, 1, length.out = 100)\nlike <- choose(n, y) * theta^y * (1 - theta)^(n - y)\ntibble(theta, like) %>%\n  ggplot(aes(x = theta, y = like)) +\n  geom_line() +\n  labs(\n    y = expression(L(theta)),\n    x = expression(\"Valori possibili di\" ~ theta)\n  )"},{"path":"ch-bayes-workflow.html","id":"sec:const-normaliz-bino23","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.6 La verosimiglianza marginale","text":"Per il calcolo di \\(p(\\theta \\mid y)\\) è necessario dividere il prodotto tra la distribuzione priori e la verosimiglianza per una costante di normalizzazione. Tale costante di normalizzazione, detta verosimiglianza marginale, ha lo scopo di fare modo che \\(p(\\theta \\mid y)\\) abbia area unitaria.Si noti che, nel caso di variabili continue, la verosimiglianza marginale è espressa nei termini di un integrale. Tranne pochi casi particolari, tale integrale non ha una soluzione analitica. Per questa ragione, l’inferenza bayesiana procede calcolando una approssimazione della distribuzione posteriori mediante metodi numerici.","code":""},{"path":"ch-bayes-workflow.html","id":"unapplicazione-empirica-2","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.6.1 Un’applicazione empirica","text":"Consideriamo nuovamente dati di Zetsche et al. (2019). Supponiamo che nel numeratore bayesiano la verosimiglianza sia moltiplicata per una distribuzione uniforme, ovvero \\(\\mbox{Beta}(1, 1)\\). tali circostanze, il prodotto si riduce alla funzione di verosimiglianza. Per dati di Zetsche et al. (2019), dunque, la costante di normalizzazione si ottiene marginalizzando la funzione di verosimiglianza \\(p(y = 23, n = 30 \\mid \\theta)\\) sopra \\(\\theta\\), ovvero risolvendo l’integrale:\\[\\begin{equation}\np(y = 23, n = 30) = \\int_0^1 \\binom{30}{23} \\theta^{23} (1-\\theta)^{7} \\,\\operatorname {d}\\!\\theta.\n\\tag{2.4}\n\\end{equation}\\]Una soluzione numerica si trova facilmente usando \\(\\R\\):La derivazione analitica è fornita nell’Appendice ??.","code":"\nlike_bin <- function(theta) {\n  choose(30, 23) * theta^23 * (1 - theta)^7\n}\nintegrate(like_bin, lower = 0, upper = 1)$value\n#> [1] 0.03225806"},{"path":"ch-bayes-workflow.html","id":"distribuzione-a-posteriori","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.7 Distribuzione a posteriori","text":"La distribuzione postreriori si trova applicando il teorema di Bayes:\\[\n\\text{probabilità posteriori} = \\frac{\\text{probabilità priori} \\cdot \\text{verosimiglianza}}{\\text{costante di normalizzazione}}\n\\]Una volta trovata la distribuzione posteriori, possiamo usarla per derivare altre quantità di interesse. Questo viene generalmente ottenuto calcolando il seguente valore atteso:\\[\nJ = \\int f(\\theta) p(\\theta \\mid y) \\,\\operatorname {d}\\!y\n\\]Se \\(f(\\cdot)\\) è la funzione identità, ad esempio, \\(J\\) risulta essere la media di \\(\\theta\\):\\[\n\\bar{\\theta} = \\int_{\\Theta} \\theta p(\\theta \\mid y) \\,\\operatorname {d}\\!\\theta .\n\\]Ripeto qui quanto detto sopra: le quantità di interesse della statistica bayesiana(costante di normalizzazione, valore atteso della distribuzione posteriori, ecc.) contengono integrali che risultano, nella maggior parte dei casi, impossibili da risolvere analiticamente. Per questo motivo, si ricorre metodi di stima numerici, particolare quei metodi Monte Carlo basati sulle proprietà delle catene di Markov (MCMC). Questo argomento verrà discusso nel Capitolo ??.","code":""},{"path":"ch-bayes-workflow.html","id":"distribuzione-predittiva-a-priori","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.8 Distribuzione predittiva a priori","text":"La distribuzione posteriori è l’oggetto centrale nella statistica bayesiana, ma non è l’unico. Oltre fare inferenze sui valori dei parametri, potremmo voler fare inferenze sui dati. Questo può essere fatto calcolando la distribuzione predittiva priori:\\[\\begin{equation}\np(y^*) = \\int_\\Theta p(y^* \\mid \\theta) p(\\theta) \\,\\operatorname {d}\\!\\theta .\n\\tag{2.5}\n\\end{equation}\\]La (2.5) descrive la distribuzione prevista dei dati base al modello (che include la distribuzione priori e la verosimiglianza), ovvero descrive dati \\(y^*\\) che ci aspettiamo di osservare, dato il modello, prima di avere osservato dati del campione.È possibile utilizzare campioni dalla distribuzione predittiva priori per valutare e calibrare modelli utilizzando le nostre conoscenze dominio-specifiche. Ad esempio, ci possiamo chiedere: “È sensato che un modello dell’altezza umana preveda che un essere umano sia alto -1.5 metri?”. Già prima di misurare una singola persona, possiamo renderci conto dell’assurdità di questa domanda. Se la distribuzione prevista dei dati consente domande di questo tipo (ovvero, prevede di osservare dati che risultano insensati alla luce delle nostre conoscenze dominio-specifiche), è chiaro che il modello deve essere riformulato.","code":""},{"path":"ch-bayes-workflow.html","id":"distribuzione-predittiva-a-posteriori","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"2.9 Distribuzione predittiva a posteriori","text":"Un’altra quantità utile da calcolare è la distribuzione predittiva posteriori:\\[\\begin{equation}\np(\\tilde{y} \\mid y) = \\int_\\Theta p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y) \\,\\operatorname {d}\\!\\theta .\n\\tag{2.6}\n\\end{equation}\\]Questa è la distribuzione dei dati attesi futuri \\(\\tilde{y}\\) alla luce della distribuzione posteriori \\(p(\\theta \\mid y)\\), che sua volta è una conseguenza del modello adottato (distribuzione priori e verosimiglianza) e dei dati osservati. altre parole, questi sono dati che il modello si aspetta dopo aver osservato dati de campione. Dalla (2.6) possiamo vedere che le previsioni sui dati attesi futuri sono calcolate integrando (o marginalizzando) sulla distribuzione posteriori dei parametri. Di conseguenza, le previsioni calcolate questo modo incorporano l’incertezza relativa alla stima dei parametri del modello.","code":""},{"path":"ch-bayes-workflow.html","id":"commenti-e-considerazioni-finali","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"Commenti e considerazioni finali","text":"Questo Capitolo ha brevemente passato rassegna concetti di base dell’inferenza statistica bayesiana. base ’approccio bayesiano, invece di dire che il parametro di interesse di un modello statistico ha un valore vero ma sconosciuto, diciamo che, prima di eseguire l’esperimento, è possibile assegnare una distribuzione di probabilità, che chiamano stato di credenza, quello che è il vero valore del parametro. Questa distribuzione priori può essere nota (per esempio, sappiamo che la distribuzione dei punteggi del QI è normale con media 100 e deviazione standard 15) o può essere del tutto arbitraria. L’inferenza bayesiana procede poi nel modo seguente: si raccolgono alcuni dati e si calcola la probabilità dei possibili valori del parametro alla luce dei dati osservati e delle credenze priori. Questa nuova distribuzione di probabilità è chiamata “distribuzione posteriori” e riassume l’incertezza dell’inferenza.","code":""},{"path":"ch-bayes-workflow.html","id":"bibliografia","chapter":"Capitolo 2 Credibilità, modelli e parametri","heading":"Bibliografia","text":"","code":""}]
