<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capitolo 2 Il modello lineare visto da vicino | Data Science per psicologi</title>
  <meta name="description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  <meta name="generator" content="bookdown 0.26.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Capitolo 2 Il modello lineare visto da vicino | Data Science per psicologi" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capitolo 2 Il modello lineare visto da vicino | Data Science per psicologi" />
  
  <meta name="twitter:description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  

<meta name="author" content="Corrado Caudek" />


<meta name="date" content="2022-04-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regr-models-intro.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data science per psicologi</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefazione</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#la-psicologia-e-la-data-science"><i class="fa fa-check"></i>La psicologia e la Data science</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#come-studiare"><i class="fa fa-check"></i>Come studiare</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sviluppare-un-metodo-di-studio-efficace"><i class="fa fa-check"></i>Sviluppare un metodo di studio efficace</a></li>
</ul></li>
<li class="part"><span><b>I Il modello lineare</b></span></li>
<li class="chapter" data-level="1" data-path="regr-models-intro.html"><a href="regr-models-intro.html"><i class="fa fa-check"></i><b>1</b> Introduzione</a>
<ul>
<li class="chapter" data-level="1.1" data-path="regr-models-intro.html"><a href="regr-models-intro.html#la-funzione-lineare"><i class="fa fa-check"></i><b>1.1</b> La funzione lineare</a></li>
<li class="chapter" data-level="1.2" data-path="regr-models-intro.html"><a href="regr-models-intro.html#una-media-per-ciascuna-osservazione"><i class="fa fa-check"></i><b>1.2</b> Una media per ciascuna osservazione</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="regr-models-intro.html"><a href="regr-models-intro.html#relazione-lineare-tra-la-media-y-mid-x-e-il-predittore"><i class="fa fa-check"></i><b>1.2.1</b> Relazione lineare tra la media <span class="math inline">\(y \mid x\)</span> e il predittore</a></li>
<li class="chapter" data-level="1.2.2" data-path="regr-models-intro.html"><a href="regr-models-intro.html#il-modello-lineare"><i class="fa fa-check"></i><b>1.2.2</b> Il modello lineare</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regr-models-intro.html"><a href="regr-models-intro.html#commenti-e-considerazioni-finali"><i class="fa fa-check"></i>Commenti e considerazioni finali</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regr-model-lm.html"><a href="regr-model-lm.html"><i class="fa fa-check"></i><b>2</b> Il modello lineare visto da vicino</a>
<ul>
<li class="chapter" data-level="2.1" data-path="regr-model-lm.html"><a href="regr-model-lm.html#stima-dei-coefficienti-di-regressione"><i class="fa fa-check"></i><b>2.1</b> Stima dei coefficienti di regressione</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="regr-model-lm.html"><a href="regr-model-lm.html#trasformazione-dei-dati"><i class="fa fa-check"></i><b>2.1.1</b> Trasformazione dei dati</a></li>
<li class="chapter" data-level="2.1.2" data-path="regr-model-lm.html"><a href="regr-model-lm.html#il-metodo-dei-minimi-quadrati"><i class="fa fa-check"></i><b>2.1.2</b> Il metodo dei minimi quadrati</a></li>
<li class="chapter" data-level="2.1.3" data-path="regr-model-lm.html"><a href="regr-model-lm.html#il-coefficiente-di-determinazione"><i class="fa fa-check"></i><b>2.1.3</b> Il coefficiente di determinazione</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="regr-model-lm.html"><a href="regr-model-lm.html#indice-di-determinazione"><i class="fa fa-check"></i><b>2.2</b> Indice di determinazione</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="regr-model-lm.html"><a href="regr-model-lm.html#inferenza-sul-modello-di-regressione"><i class="fa fa-check"></i><b>2.2.1</b> Inferenza sul modello di regressione</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regr-model-lm.html"><a href="regr-model-lm.html#commenti-e-considerazioni-finali-1"><i class="fa fa-check"></i>Commenti e considerazioni finali</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science per psicologi</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regr-model-lm" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Capitolo 2</span> Il modello lineare visto da vicino<a href="regr-model-lm.html#regr-model-lm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In questo capitolo mi pongo il problema di applicare il modello di regressione bivariata ad un campione di dati. Userò i dati <code>kidiq</code>. Riporto qui di seguito la descrizione di questo set di dati.</p>
<blockquote>
<p>Data from a survey of adult American women and their children (a subsample from the National Longitudinal Survey of Youth).</p>
</blockquote>
<blockquote>
<p>Source: Gelman and Hill (2007)</p>
</blockquote>
<blockquote>
<p>434 obs. of 4 variables</p>
</blockquote>
<blockquote>
<ul>
<li>kid_score Child’s IQ score</li>
<li>mom_hs Indicator for whether the mother has a high school degree</li>
<li>mom_iq Mother’s IQ score</li>
<li>mom_age Mother’s age</li>
</ul>
</blockquote>
<p>Leggo i dati in <span class="math inline">\(\mathsf{R}\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="regr-model-lm.html#cb1-1" aria-hidden="true" tabindex="-1"></a>kidiq <span class="ot">&lt;-</span> rio<span class="sc">::</span><span class="fu">import</span>(here<span class="sc">::</span><span class="fu">here</span>(</span>
<span id="cb1-2"><a href="regr-model-lm.html#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;data&quot;</span>, <span class="st">&quot;kidiq.dta&quot;</span></span>
<span id="cb1-3"><a href="regr-model-lm.html#cb1-3" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb1-4"><a href="regr-model-lm.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(kidiq)</span>
<span id="cb1-5"><a href="regr-model-lm.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Rows: 434</span></span>
<span id="cb1-6"><a href="regr-model-lm.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Columns: 5</span></span>
<span id="cb1-7"><a href="regr-model-lm.html#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; $ kid_score &lt;dbl&gt; 65, 98, 85, 83, 115, 98, 69, 106, 102, 95, 91, 58, 84, 78, 1…</span></span>
<span id="cb1-8"><a href="regr-model-lm.html#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; $ mom_hs    &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, …</span></span>
<span id="cb1-9"><a href="regr-model-lm.html#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; $ mom_iq    &lt;dbl&gt; 121.11753, 89.36188, 115.44316, 99.44964, 92.74571, 107.9018…</span></span>
<span id="cb1-10"><a href="regr-model-lm.html#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; $ mom_work  &lt;dbl&gt; 4, 4, 4, 3, 4, 1, 4, 3, 1, 1, 1, 4, 4, 4, 2, 1, 3, 3, 4, 3, …</span></span>
<span id="cb1-11"><a href="regr-model-lm.html#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; $ mom_age   &lt;dbl&gt; 27, 25, 27, 25, 27, 18, 20, 23, 24, 19, 23, 24, 27, 26, 24, …</span></span></code></pre></div>
<p>In questo esercizio considererò la relazione tra <code>kid_score</code> e <code>mom_iq</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="regr-model-lm.html#cb2-1" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb2-2"><a href="regr-model-lm.html#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> kid_score)) <span class="sc">+</span></span>
<span id="cb2-3"><a href="regr-model-lm.html#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>I dati rappresentati nel diagramma a dispersione suggeriscono che, in questo campione, sembra esserci un’associazione positiva tra l’intelligenza del bambino (kid_score) e l’intelligenza della madre (mom_iq). Mi pongo il problema di descrivere questa associazione mediante una relazione lineare.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="regr-model-lm.html#cb3-1" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb3-2"><a href="regr-model-lm.html#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> kid_score)) <span class="sc">+</span> </span>
<span id="cb3-3"><a href="regr-model-lm.html#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb3-4"><a href="regr-model-lm.html#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-6-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Ci sono infinite rette che, in linea di principio, possono essere usate per “approssimare” la nube di punti nel diagramma a dispersione. È dunque necessario introdurre dei vincoli per selezionare una di queste rette.</p>
<p>Un vincolo che viene spesso usato è quello di costringere la retta a passare per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="regr-model-lm.html#cb4-1" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb4-2"><a href="regr-model-lm.html#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> kid_score)) <span class="sc">+</span> </span>
<span id="cb4-3"><a href="regr-model-lm.html#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb4-4"><a href="regr-model-lm.html#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb4-5"><a href="regr-model-lm.html#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb4-6"><a href="regr-model-lm.html#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x=</span><span class="fu">mean</span>(mom_iq), <span class="at">y=</span><span class="fu">mean</span>(kid_score)), </span>
<span id="cb4-7"><a href="regr-model-lm.html#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour=</span><span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">4</span></span>
<span id="cb4-8"><a href="regr-model-lm.html#cb4-8" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-7-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Una retta che passa per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> ha la proprietà descritta di seguito.</p>
<p>Iniziamo a descrivere ciascuna osservazione inserita nel diagramma a dispersione con un’equazione:</p>
<p><span class="math display">\[
y_i = a + b x_i + e_i
\]</span>
Le osservazioni <span class="math inline">\(y\)</span> sono <code>kid_score</code>. I primi 10 valori sono i seguenti:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="regr-model-lm.html#cb5-1" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>kid_score[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb5-2"><a href="regr-model-lm.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1]  65  98  85  83 115  98  69 106 102  95</span></span></code></pre></div>
<p>Per fare riferimento a ciascun valore usiamo l’indice <span class="math inline">\(i\)</span>. Quindi, ad esempio, <span class="math inline">\(y_3\)</span> è uguale a</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="regr-model-lm.html#cb6-1" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>kid_score[<span class="dv">3</span>]</span>
<span id="cb6-2"><a href="regr-model-lm.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 85</span></span></code></pre></div>
<p>La variabile <span class="math inline">\(x\)</span>, nel caso presente, è <code>mom_iq</code>. I primi 10 valori di <span class="math inline">\(x\)</span> sono</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="regr-model-lm.html#cb7-1" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>mom_iq[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb7-2"><a href="regr-model-lm.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] 121.11753  89.36188 115.44316  99.44964  92.74571 107.90184 138.89311</span></span>
<span id="cb7-3"><a href="regr-model-lm.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [8] 125.14512  81.61953  95.07307</span></span></code></pre></div>
<p>In maniera corrispondente alla <span class="math inline">\(y\)</span>, uso un indice per fare riferimento ai singoli valori della variabile. Ad esempio, <span class="math inline">\(x_3\)</span> è</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="regr-model-lm.html#cb8-1" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>mom_iq[<span class="dv">3</span>]</span>
<span id="cb8-2"><a href="regr-model-lm.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 115.4432</span></span></code></pre></div>
<p>L’equazione precedente ci dice che ciascun valore <span class="math inline">\(y\)</span> è dato dalla somma di due componenti: una componente deterministica e una componente aleatoria. Consideriamo il primo valore <span class="math inline">\(y\)</span>. Per esso diciamo che</p>
<p><span class="math display">\[
y_1 = a + b x_1 + e_1,
\]</span></p>
<p>laddove <span class="math inline">\(a + b x_1\)</span> è la componente deterministica, detta <span class="math inline">\(\hat{y}\)</span>, e <span class="math inline">\(e_1\)</span> è la componente aleatoria.</p>
<p>La componente deterministica è, appunto, la componente di ciascun valore <span class="math inline">\(y\)</span> che possiamo prevedere conoscendo <span class="math inline">\(x\)</span>. Non possiamo prevedere perfettamente i valori <span class="math inline">\(y\)</span> – ciò si verificherebbe soltanto se tutti punti del diagramma a dispersione fossero disposti su una retta. Ma non lo sono: la retta è solo un’approssimazione della relazione (lineare) tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Pertanto, conoscendo <span class="math inline">\(x\)</span> possiamo solo prevedere una “componente” di ciascun valore <span class="math inline">\(y\)</span>.</p>
<p>Cosa significa che possiamo prevedere una componente di ciascuna osservazione <span class="math inline">\(y\)</span>? Significa che il valore <span class="math inline">\(y\)</span> osservato sarà dato dalla somma di due componenti: <span class="math inline">\(y_i = \hat{y}_i + e_i\)</span>.</p>
<p>L’affermazione precedente solleva due domande:</p>
<ul>
<li>come possiamo trovare la quota della <span class="math inline">\(y\)</span> che può essere predetta conoscendo <span class="math inline">\(x\)</span>?</li>
<li>quant’è grande la porzione della <span class="math inline">\(y\)</span> che può essere predetta conoscendo <span class="math inline">\(x\)</span>? In altre parole, conoscendo la <span class="math inline">\(x\)</span> è possibile predire <span class="math inline">\(y\)</span> con accuratezza oppure no?</li>
</ul>
<p>Rispondere a tali due domanda definisce i primi due obiettivi del modello statistico della regressione lineare. Il terzo obiettivo è quello dell’inferenza, ovvero di capire che relazioni ci sono tra la relazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> osservata nel campione e la la relazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> nella popolazione.</p>
<div id="stima-dei-coefficienti-di-regressione" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Stima dei coefficienti di regressione<a href="regr-model-lm.html#stima-dei-coefficienti-di-regressione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Iniziamo con il primo obiettivo, ovvero quello di predire una frazione di ciascuna osservazione <span class="math inline">\(y\)</span> conoscendo <span class="math inline">\(x\)</span>. Quindi, nel caso presente, ci chiediamo quanto segue. Il primo bambino del campione ha un QI uguale a 65. Sua madre ha un QI di 121.12. Quanto bene riesco a predire il punteggio QI del bambino conoscendo quello di sua madre?</p>
<p>È chiaro, guardando i numeri, che non c’è una corrispondenza perfetta, tutt’altro! Infatti, se guardiamo il diagramma di dispersione ci rendiamo conto che i punti sono piuttosto lontani dalla retta che abbiamo usato per descrivere la relazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Tuttavia, il diagramma di dispersione suggerisce che una qualche relazione c’è, seppur debole. Il nostro obiettivo è di descrivere una tale relazione.</p>
<p>Una tale relazione è descritta dalla componente deterministica che costituisce una frazione di ciascuna osservazione <span class="math inline">\(y\)</span>. Abbiamo deciso di definire una tale componente “deterministica” <span class="math inline">\(\hat{y}_i\)</span> nei termini della seguente equazione: <span class="math inline">\(\hat{y}_i = a_i + bx_i\)</span>.</p>
<p>L’equazione precedente è detta equazione lineare e, dal punto di vista geometrico, corrisponde ad una retta. Ci sono infinite equazioni che potremmo usare per mettere in relazione <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>. Abbiamo scelto questa perché è la più semplice. Se guardiamo il diagramma di dispersione, infatti, non ci sono ragioni per descrivere la relazione tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> con qualche curva, anziché con una retta. In altri campioni, una curva può essere più sensata di una retta, quale descrizione della relazione <em>media</em> tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, ma non nel caso presente. Ricordiamo il principio del rasoio di Occam (ovvero, il principio che sta alla base del pensiero scientifico moderno): se un modello semplice funziona, non c’è ragione di usare un modello più complesso.</p>
<p>Dunque, abbiamo capito che vogliamo descrivere la relazione media* tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> con una retta, ovvero, con l’equazione lineare</p>
<p><span class="math display">\[
\hat{y}_i = a + b x_i.
\]</span></p>
<p>L’equazione precedente indica che il modello lineare <span class="math inline">\(a + b x_i\)</span> <em>non è in grado di prevedere</em> il valore di ciascuna osservazione <span class="math inline">\(y_i\)</span>. Questo, in generale, non è mai possibile (ovvero, è possibile solo in un caso specifico che, nella realtà empirica, non si verifica mai).</p>
<p>L’equazione precedente ci dice che possiamo prevedere solo una frazione di ciascuna osservazione <span class="math inline">\(y_i\)</span>, ovvero quella frazione che abbiamo denotato con <span class="math inline">\(\hat{y}_i\)</span>. La componente che non possiamo prevedere con l’equazione <span class="math inline">\(a + b x_i\)</span> si denota con <span class="math inline">\(e_i\)</span>.</p>
<p>In questo senso diciamo che <em>scomponiamo</em> il valore di ciascuna osservazione <span class="math inline">\(y_i\)</span> in due componenti: la componente deterministica (prevedibile conoscendo <span class="math inline">\(x\)</span>) e la componente aleatoria (non prevedibile conoscendo <span class="math inline">\(x\)</span>):</p>
<p><span class="math display">\[
y_i = \hat{y}_i + e_i.
\]</span></p>
<p>Il primo obiettivo del modello di regressione è quello di trovare i coefficienti dell’equazione</p>
<p><span class="math display">\[
a + b x_i
\]</span></p>
<p>che consente di predire <span class="math inline">\(\hat{y}_i\)</span>. Questi due coefficienti sono detti <em>coefficienti di regressione</em>.</p>
<p>Per trovare i coefficienti di regressione dobbiamo introdurre dei vincoli per limitare lo spazio delle possibili soluzioni.</p>
<p>Il primo vincolo lo abbiamo espresso prima: vogliamo che la retta <span class="math inline">\(\hat{y}_i = a + b x_i\)</span> passi per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>. Il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> corrisponde al <em>baricentro</em> del diagramma a dispersione.</p>
<p>Ci sono infinite rette che passano per i punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>. Tutte queste rette soddisfano la seguente proprietà. Nel caso di qualsiasi retta passante per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> è vero che</p>
<p><span class="math display">\[
\sum_{i=1}^n e_i = 0.
\]</span></p>
<p>Dal punto di vista geometrico, la componente erratica del modello, <span class="math inline">\(e_i\)</span>, corrisponde alla distanza verticale tra ciascun punto e la retta di regressione <span class="math inline">\(a + bx\)</span>. Tale componente va sotto il nome di <em>residuo</em>:</p>
<p><span class="math display">\[
e_i = y_i - \hat{y}_i = y_i - (a + bx_i).
\]</span></p>
<p>L’affermazione precedente dice che la somma di tutte le distanze verticali (che hanno un segno positivo quando il punto è sopra la retta, e un segno negativo quando il punto è sotto la retta) tra le osservazioni e la retta di regressione (passante per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>) è uguale a zero.</p>
<p>Questo significa che non possiamo selezionare una tra le infinite rette che passano per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> usando il criterio che ci porta a scegliere la retta che rende la più piccola possibile (ovvero, minimizza) la somma dei residui. Infatti, tutte le rette passanti per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span> rendono uguale a zero la somma dei residui.</p>
<p>Dunque, dobbiamo trovare qualche altri criterio per scegliere tra le infinite rette che passano per il punto <span class="math inline">\((\bar{x}, \bar{y})\)</span>. Il criterio che viene normalmente scelto è quello di minimizzare la somma dei quadrati dei residui <span class="math inline">\((y_i - \hat{y}_i)^2\)</span>. In altri termini, vogliamo trovare i coefficienti <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> tali per cui la quantità</p>
<p><span class="math display">\[
\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}
\]</span></p>
<p>assume il suo valore minimo. I coefficienti che hanno questa proprietà si chiamano <em>coefficienti dei minimi quadrati</em>.</p>
<p>Questo problema ha una soluzione analitica. La soluzione analitica si trova riconoscendo il fatto che l’equazione precedente definisce una superficie e il problema diventa quello di trovare il punto minore di questa superficie. Per trovare la soluzione ci si deve rendere conto che il punto di minimo è il punto della superficie nel quale la tangente alla superficie è piatta (ovvero uguale a zero). Rendere uguale a zero la tangente ad una superficie significa porre le derivate parziali rispetto alla direzione <span class="math inline">\(x\)</span> e alla direzione <span class="math inline">\(y\)</span> uguali a zero. Ponendo tali derivate parziali uguali a zero si definisce un sistema di equazioni lineari con due incognite, <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>. La soluzione di tali equazioni, che si chiamano <em>equazioni normali</em>, è la seguente:</p>
<p><span class="math display">\[
a = \bar{y} - b \bar{x}
\]</span></p>
<p><span class="math display">\[
b = \frac{\mbox{Cov}(x, y)}{\mbox{Var}(x)}
\]</span></p>
<p>Le due precedenti equazioni corrispondono alla stima dei minimi quadrati dei coefficienti di regressione della retta che minimizza la somma dei quadrati dei residui.</p>
<p>Nel caso presente, tali coefficienti sono uguali a:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="regr-model-lm.html#cb9-1" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">cov</span>(kidiq<span class="sc">$</span>kid_score, kidiq<span class="sc">$</span>mom_iq) <span class="sc">/</span> <span class="fu">var</span>(kidiq<span class="sc">$</span>mom_iq)</span>
<span id="cb9-2"><a href="regr-model-lm.html#cb9-2" aria-hidden="true" tabindex="-1"></a>b</span>
<span id="cb9-3"><a href="regr-model-lm.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.6099746</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="regr-model-lm.html#cb10-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>kid_score) <span class="sc">-</span> b <span class="sc">*</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>mom_iq)</span>
<span id="cb10-2"><a href="regr-model-lm.html#cb10-2" aria-hidden="true" tabindex="-1"></a>a</span>
<span id="cb10-3"><a href="regr-model-lm.html#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 25.79978</span></span></code></pre></div>
<p>In R li possiamo trovare con la seguente funzione:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="regr-model-lm.html#cb11-1" aria-hidden="true" tabindex="-1"></a>fm <span class="ot">&lt;-</span> <span class="fu">lm</span>(kid_score <span class="sc">~</span> mom_iq, <span class="at">data =</span> kidiq)</span>
<span id="cb11-2"><a href="regr-model-lm.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fm)</span>
<span id="cb11-3"><a href="regr-model-lm.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)      mom_iq </span></span>
<span id="cb11-4"><a href="regr-model-lm.html#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  25.7997778   0.6099746</span></span></code></pre></div>
<p>In precedenza abbiamo soltanto accennato al problema di come si possono trovano i coefficienti dei minimi quadrati; ritorneremo su questo punto in seguito. Per ora, chiediamoci cosa significano i due coefficienti che abbiamo calcolato.</p>
<p>Il coefficiente <span class="math inline">\(a\)</span> si chiama <em>intercetta</em>. L’intercetta, all’interno del diagramma a dispersione, specifica il punto in cui la retta di regressione interseca l’asse <span class="math inline">\(y\)</span> del sistema di assi cartesiani.</p>
<p>Nel caso presente questo valore non è di alcun interesse, perché corrisponde al valore della retta di regressione quando <span class="math inline">\(x = 0\)</span>, ovvero quando l’intelligenza della madre è uguale a 0. Vedremo in seguito come, trasformando i dati, è possibile assegnare al coefficiente <span class="math inline">\(a\)</span> un’interpretazione più utile. Per ora mi limito a fornire l’interpretazione del coefficiente.</p>
<p>Passando a <span class="math inline">\(b\)</span>, possiamo dire che questo secondo coefficiente va sotto il nome di <em>pendenza</em> della retta di regressione. Ovvero ci dice di quanto aumenta (se <span class="math inline">\(b\)</span> è positivo) o diminuisce (se <span class="math inline">\(b\)</span> è negativo) la retta di regressione in corrispondenza di un aumento di 1 punto della variabile <span class="math inline">\(x\)</span>.</p>
<p>Nel caso presente, il coefficiente <span class="math inline">\(b\)</span> ci dice che, se il QI delle madri aumenta di 1 punto, il QI dei bambini aumenta <strong>in media</strong> di 0.61 punti.</p>
<p>È importante capire cosa significa che, in base ai risultati della regressione, <span class="math inline">\(y\)</span> aumenta <em>in media</em> di <span class="math inline">\(b\)</span> punti per ciascun aumento unitario di <span class="math inline">\(x\)</span>.</p>
<p>Il modello statistico di regressione <em>ipotizza</em> che, per ciascun valore osservato <span class="math inline">\(x\)</span> (per esempio, il valore del QI della prima madre del campione, ovvero <span class="math inline">\(x = 121.11753\)</span>) ci sia una distribuzione di valori <span class="math inline">\(y\)</span> nella popolazione, di cui solo uno è stato osservato nel campione. Possiamo facilmente capire che, se consideriamo tutte le madri con QI di 121.12, il punteggio del QI dei loro figli non sia costante, ma assuma tanti valori possibili. Questa distribuzione di valori possibili si chiama distribuzione <span class="math inline">\(y\)</span> condizionata a <span class="math inline">\(x\)</span>, ovvero <span class="math inline">\(p(y \mid x_i)\)</span>.</p>
<p>Il modello statistico della regressione lineare non può in alcun modo prevedere il valore assunto da ciascuna delle possibili osservazioni che fanno parte della distribuzione <span class="math inline">\(p(y \mid x_i)\)</span>. Il modello della regressione lineare ha un obiettivo più limitato, ovvero si propone di prevedere <em>le medie</em> delle distribuzioni <span class="math inline">\(p(y \mid x_i)\)</span> conoscendo i valori <span class="math inline">\(x\)</span>.</p>
<p>Dunque, quando il coefficiente <span class="math inline">\(b\)</span> è uguale a 0.61, questo significa che il modello di regressione predice che <em>la medie</em> della distribuzione condizionata <span class="math inline">\(p(y \mid x_i)\)</span> aumenta di 0.61 punti se la variabile <span class="math inline">\(x\)</span> (QI delle madri) aumenta di un punto. Questo significa che il modello di regressione non fa una predizione sul punteggio di ciascun valore <span class="math inline">\(y_i\)</span> (in funzione di <span class="math inline">\(x\)</span>), ma solo della media delle distribuzioni condizionate <span class="math inline">\(p(y \mid x_i)\)</span> di cui il valore osservato <span class="math inline">\(y_i\)</span> è una realizzazione casuale.</p>
<p>Possiamo dire la stessa cosa con parole diverse dicendo che il modello di regressione fa delle predizioni sulla componente deterministica di ciascuna osservazione. È più semplice capire questo aspetto se rappresentiamo in maniera grafica la componente “deterministica” <span class="math inline">\(\hat{y}_i = a + b x_i\)</span> predetta dal modello di regressione.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="regr-model-lm.html#cb12-1" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>yhat <span class="ot">&lt;-</span> fm<span class="sc">$</span>fitted.values</span>
<span id="cb12-2"><a href="regr-model-lm.html#cb12-2" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb12-3"><a href="regr-model-lm.html#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> mom_iq, <span class="at">y =</span> yhat)) <span class="sc">+</span> </span>
<span id="cb12-4"><a href="regr-model-lm.html#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb12-5"><a href="regr-model-lm.html#cb12-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb12-6"><a href="regr-model-lm.html#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb12-7"><a href="regr-model-lm.html#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x=</span><span class="fu">mean</span>(mom_iq), <span class="at">y=</span><span class="fu">mean</span>(kid_score)), </span>
<span id="cb12-8"><a href="regr-model-lm.html#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour=</span><span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">4</span></span>
<span id="cb12-9"><a href="regr-model-lm.html#cb12-9" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-15-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Il diagramma precedente presenta ciascun valore <span class="math inline">\(\hat{y}_i = a + b x_i\)</span> in funzione di <span class="math inline">\(x_i\)</span>. Si vede che i valori predetti dal modello di regressione sono i punti che stanno sulla retta di regressione. Dunque, il residuo, ovvero la componente di ciascuna osservazione <span class="math inline">\(y_i\)</span> che non viene predetta dal modello di regressione corrisponde alla distanza verticale tra i punti del diagramma a dispersione e la retta di regressione</p>
<p><span class="math display">\[
e_i = y_i - (a + b x_i).
\]</span></p>
<p>Nel caso nella prima osservazione, ad esempio abbiamo:</p>
<p><span class="math display">\[
y_1 = (a + b x_1) + e_1
\]</span></p>
<p>Abbiamo</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="regr-model-lm.html#cb13-1" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>kid_score[<span class="dv">1</span>]</span>
<span id="cb13-2"><a href="regr-model-lm.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 65</span></span></code></pre></div>
<p>Dunque</p>
<p><span class="math display">\[
e_1 = (a + b x_1) - y_1
\]</span></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="regr-model-lm.html#cb14-1" aria-hidden="true" tabindex="-1"></a>e_1 <span class="ot">&lt;-</span> kidiq<span class="sc">$</span>kid_score[<span class="dv">1</span>] <span class="sc">-</span> (a <span class="sc">+</span> b <span class="sc">*</span> kidiq<span class="sc">$</span>mom_iq[<span class="dv">1</span>])</span>
<span id="cb14-2"><a href="regr-model-lm.html#cb14-2" aria-hidden="true" tabindex="-1"></a>e_1</span>
<span id="cb14-3"><a href="regr-model-lm.html#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] -34.67839</span></span></code></pre></div>
<p>Ciò significa che il valore osservato <span class="math inline">\(y_1 = 65\)</span> viene scomposto dal modello di regressione in due componenti. La componente deterministica <span class="math inline">\(\hat{y}_1\)</span>, predicibile da <span class="math inline">\(x_1\)</span>, è</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="regr-model-lm.html#cb15-1" aria-hidden="true" tabindex="-1"></a>yhat_1 <span class="ot">&lt;-</span> a <span class="sc">+</span> b <span class="sc">*</span> kidiq<span class="sc">$</span>mom_iq[<span class="dv">1</span>]</span>
<span id="cb15-2"><a href="regr-model-lm.html#cb15-2" aria-hidden="true" tabindex="-1"></a>yhat_1</span>
<span id="cb15-3"><a href="regr-model-lm.html#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 99.67839</span></span></code></pre></div>
<p>La somma della componente deterministica e della componente erratica, ovviamente, riproduce il valore osservato.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="regr-model-lm.html#cb16-1" aria-hidden="true" tabindex="-1"></a>yhat_1 <span class="sc">+</span> e_1</span>
<span id="cb16-2"><a href="regr-model-lm.html#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 65</span></span></code></pre></div>
<div id="trasformazione-dei-dati" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Trasformazione dei dati<a href="regr-model-lm.html#trasformazione-dei-dati" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consideriamo ora i dati <span class="math inline">\(y\)</span> espressi come differenze dalla media:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="regr-model-lm.html#cb17-1" aria-hidden="true" tabindex="-1"></a>kidiq<span class="sc">$</span>xd <span class="ot">&lt;-</span> kidiq<span class="sc">$</span>mom_iq <span class="sc">-</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>mom_iq)</span></code></pre></div>
<p>Il diagramma a dispersione diventa il seguente.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="regr-model-lm.html#cb18-1" aria-hidden="true" tabindex="-1"></a>kidiq <span class="sc">%&gt;%</span> </span>
<span id="cb18-2"><a href="regr-model-lm.html#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> xd, <span class="at">y =</span> kid_score)) <span class="sc">+</span> </span>
<span id="cb18-3"><a href="regr-model-lm.html#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb18-4"><a href="regr-model-lm.html#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb18-5"><a href="regr-model-lm.html#cb18-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb18-6"><a href="regr-model-lm.html#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x=</span><span class="fu">mean</span>(xd), <span class="at">y=</span><span class="fu">mean</span>(kid_score)), </span>
<span id="cb18-7"><a href="regr-model-lm.html#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour=</span><span class="st">&quot;red&quot;</span>, <span class="at">size =</span> <span class="dv">4</span></span>
<span id="cb18-8"><a href="regr-model-lm.html#cb18-8" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-21-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Nel diagramma precedente, la pendenza della retta di regressione è uguale alla precedente, ma in questo grafico all’intercetta può essere assegnata un’interpretazione dotata di senso.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="regr-model-lm.html#cb19-1" aria-hidden="true" tabindex="-1"></a>fm1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(kid_score <span class="sc">~</span> xd, <span class="at">data =</span> kidiq)</span>
<span id="cb19-2"><a href="regr-model-lm.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fm1)</span>
<span id="cb19-3"><a href="regr-model-lm.html#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept)          xd </span></span>
<span id="cb19-4"><a href="regr-model-lm.html#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  86.7972350   0.6099746</span></span></code></pre></div>
<p>Nel caso di dati così trasformati, l’intercetta è sempre il punto sull’asse <span class="math inline">\(y\)</span> dove la retta di regressione interseca l’ordinata. Ma, in questo caso, dato che abbiamo traslato i dati di una quantità pari a <span class="math inline">\(x - \bar{x}\)</span>, il valore <span class="math inline">\(x = 0\)</span> corrisponde al valore <span class="math inline">\(\bar{x}\)</span> nel caso dei dati grezzi. Dunque, l’intercetta avrà la seguente interpretazione:</p>
<p>nel caso di dati nei quali <span class="math inline">\(x\)</span> è espresso come differenze dalla media, l’intercetta corrisponde al valore atteso della <span class="math inline">\(y\)</span> in corrispondenza di <span class="math inline">\(\bar{x}\)</span>.</p>
<p>In altre parole, per i dati così trasformati, l’intercetta corrisponde al QI atteso (ovvero, medio) dei bambini in corrispondenza del QI medio delle madri.</p>
</div>
<div id="il-metodo-dei-minimi-quadrati" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Il metodo dei minimi quadrati<a href="regr-model-lm.html#il-metodo-dei-minimi-quadrati" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ora che abbiamo visto come interpretare il coefficienti di regressione, chiediamoci come vengono calcolati.</p>
<p>La procedura generale è stata brevemente descritta in precedenza. Vediamo ora come si giunge alla stessa conclusione usando una simulazione.</p>
<p>Il problema è di trovare i valori <span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span> tali per cui la quantità <span class="math inline">\(\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}\)</span> assume il valore minore possibile. Questo è un problema di minimizzazione rispetto a due parametri. Per dare un’idea di come si fa, semplifichiamo il problema e supponiamo che uno dei due parametri sia noto, ad esempio <span class="math inline">\(a\)</span>, così ci resta una sola incognita.</p>
<p>Credo una griglia di valori <code>b_grid</code> possibili, ad esempio:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="regr-model-lm.html#cb20-1" aria-hidden="true" tabindex="-1"></a>nrep <span class="ot">&lt;-</span> <span class="fl">1e5</span></span>
<span id="cb20-2"><a href="regr-model-lm.html#cb20-2" aria-hidden="true" tabindex="-1"></a>b_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> nrep)</span></code></pre></div>
<p>Definisco una funzione che calcola la quantità <span class="math inline">\(\sum_{i=1}^{n}{(y_i - (a + b x_i))^2}\)</span>:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="regr-model-lm.html#cb21-1" aria-hidden="true" tabindex="-1"></a>sse <span class="ot">&lt;-</span> <span class="cf">function</span>(a, b, x, y) {</span>
<span id="cb21-2"><a href="regr-model-lm.html#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>((y <span class="sc">-</span> (a <span class="sc">+</span> b <span class="sc">*</span> x))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb21-3"><a href="regr-model-lm.html#cb21-3" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Calcolo la somma degli errori quadratici per ciascun possibile valore <code>b_grid</code>, fissando <span class="math inline">\(a = 25.79978\)</span>.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="regr-model-lm.html#cb22-1" aria-hidden="true" tabindex="-1"></a>sse_res <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, nrep)</span>
<span id="cb22-2"><a href="regr-model-lm.html#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nrep) {</span>
<span id="cb22-3"><a href="regr-model-lm.html#cb22-3" aria-hidden="true" tabindex="-1"></a>  sse_res[i] <span class="ot">&lt;-</span> <span class="fu">sse</span>(<span class="at">a =</span> <span class="fl">25.79978</span>, <span class="at">b =</span> b_grid[i], <span class="at">x =</span> kidiq<span class="sc">$</span>mom_iq, <span class="at">y =</span> kidiq<span class="sc">$</span>kid_score)</span>
<span id="cb22-4"><a href="regr-model-lm.html#cb22-4" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Esaminiamo il risultato ottenuto.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="regr-model-lm.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb23-2"><a href="regr-model-lm.html#cb23-2" aria-hidden="true" tabindex="-1"></a>  b_grid, sse_res, <span class="at">type =</span> <span class="st">&#39;l&#39;</span></span>
<span id="cb23-3"><a href="regr-model-lm.html#cb23-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-26-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Il risultato ottenuto con la simulazione</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="regr-model-lm.html#cb24-1" aria-hidden="true" tabindex="-1"></a>b_grid[<span class="fu">which.min</span>(sse_res)]</span>
<span id="cb24-2"><a href="regr-model-lm.html#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.6099761</span></span></code></pre></div>
<p>riproduce quello ottenuto per via analitica:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="regr-model-lm.html#cb25-1" aria-hidden="true" tabindex="-1"></a>b</span>
<span id="cb25-2"><a href="regr-model-lm.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.6099746</span></span></code></pre></div>
<p>Una simulazione simile, ma computazionalmente più complessa, può essere usata per stimare simultaneamente entrambi i parametri. Ci siamo limitati qui ad una <em>proof of concept</em> del caso più semplice.</p>
</div>
<div id="il-coefficiente-di-determinazione" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Il coefficiente di determinazione<a href="regr-model-lm.html#il-coefficiente-di-determinazione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Il secondo obiettivo del modello statistico di regressione lineare è quello di stabilire <em>quanto sia grande, in termini proporzionali, la componente <span class="math inline">\(y\)</span> predicibile da <span class="math inline">\(x\)</span>, per ciascuna osservazione</em>.</p>
<p>Un indice assoluto della bontà di adattamento è fornito dalla deviazione standard dei residui, <span class="math inline">\(s_e\)</span>, chiamata anche <em>errore standard della stima</em>. Uno stimatore non distorto della varianza dei residui nella popolazione è dato da</p>
<p><span class="math display">\[
s^2_e = \frac{\sum e_i^2}{n-2}
\]</span></p>
<p>e quindi l’errore standard della stima sarà</p>
<p><span class="math display">\[\begin{equation}
s_e = \sqrt{\frac{\sum e_i^2}{n-2}}.
\end{equation}\]</span></p>
<p>Si noti che questa è la stessa formula della varianza (dato che la media dei residui è zero), tranne per il fatto che al denominatore abbiamo <span class="math inline">\(n-2\)</span>. Dato che, per calcolare <span class="math inline">\(\hat{y}\)</span> abbiamo usato due coefficienti (<span class="math inline">\(a\)</span> e <span class="math inline">\(b\)</span>), si dice che “abbiamo perso due gradi di libertà”.</p>
<p>Dato che <span class="math inline">\(s_e\)</span> possiede la stessa unità di misura della variabile <span class="math inline">\(y\)</span>, l’errore standard della stima può essere considerato come una sorta di “residuo medio.” – usando la stessa interpretazione che diamo alla deviazione standard in generale.</p>
<p>Si noti che la formula precedente non fornisce la “deviazione standard dei residui nel campione” (quella formula avrebbe <span class="math inline">\(n\)</span> al denominatore). Invece, fornisce una <em>stima</em> della deviazione standard dei residui nella popolazione da cui il campione è stato estratto.</p>
<p>Verifichiamo quanto detto con i dati a disposizione.</p>
<p>I residui possono essere trovati nel modo seguente.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="regr-model-lm.html#cb26-1" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> kidiq<span class="sc">$</span>kid_score <span class="sc">-</span> (a <span class="sc">+</span> b <span class="sc">*</span> kidiq<span class="sc">$</span>mom_iq)</span>
<span id="cb26-2"><a href="regr-model-lm.html#cb26-2" aria-hidden="true" tabindex="-1"></a>e[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb26-3"><a href="regr-model-lm.html#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [1] -34.678390  17.691747 -11.217173  -3.461529  32.627697   6.382845</span></span>
<span id="cb26-4"><a href="regr-model-lm.html#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  [7] -41.521041   3.864881  26.414387  11.208068</span></span></code></pre></div>
<p>Oppure nel modo seguente.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="regr-model-lm.html#cb27-1" aria-hidden="true" tabindex="-1"></a>fm<span class="sc">$</span>residuals[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb27-2"><a href="regr-model-lm.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;          1          2          3          4          5          6          7 </span></span>
<span id="cb27-3"><a href="regr-model-lm.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -34.678390  17.691747 -11.217173  -3.461529  32.627697   6.382845 -41.521041 </span></span>
<span id="cb27-4"><a href="regr-model-lm.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;          8          9         10 </span></span>
<span id="cb27-5"><a href="regr-model-lm.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3.864881  26.414387  11.208068</span></span></code></pre></div>
<p>Calcolo il residuo medio, prendendo il valore assoluto.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="regr-model-lm.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(e))</span>
<span id="cb28-2"><a href="regr-model-lm.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 14.4686</span></span></code></pre></div>
<p>L’errore standard della regressione è</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="regr-model-lm.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(e<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (<span class="fu">length</span>(e) <span class="sc">-</span> <span class="dv">2</span>))</span>
<span id="cb29-2"><a href="regr-model-lm.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 18.26612</span></span></code></pre></div>
<p>I due numeri non sono uguali, ma possiamo dire che hanno lo stesso ordine di grandezza.</p>
<p>Se usiamo la funzione <code>lm()</code> otteniamo lo stesso valore, chiamato <code>Residual standard error</code>.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="regr-model-lm.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm)</span>
<span id="cb30-2"><a href="regr-model-lm.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb30-3"><a href="regr-model-lm.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb30-4"><a href="regr-model-lm.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = kid_score ~ mom_iq, data = kidiq)</span></span>
<span id="cb30-5"><a href="regr-model-lm.html#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb30-6"><a href="regr-model-lm.html#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb30-7"><a href="regr-model-lm.html#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb30-8"><a href="regr-model-lm.html#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -56.753 -12.074   2.217  11.710  47.691 </span></span>
<span id="cb30-9"><a href="regr-model-lm.html#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb30-10"><a href="regr-model-lm.html#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb30-11"><a href="regr-model-lm.html#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb30-12"><a href="regr-model-lm.html#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 25.79978    5.91741    4.36 1.63e-05 ***</span></span>
<span id="cb30-13"><a href="regr-model-lm.html#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; mom_iq       0.60997    0.05852   10.42  &lt; 2e-16 ***</span></span>
<span id="cb30-14"><a href="regr-model-lm.html#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb30-15"><a href="regr-model-lm.html#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb30-16"><a href="regr-model-lm.html#cb30-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb30-17"><a href="regr-model-lm.html#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 18.27 on 432 degrees of freedom</span></span>
<span id="cb30-18"><a href="regr-model-lm.html#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.201,  Adjusted R-squared:  0.1991 </span></span>
<span id="cb30-19"><a href="regr-model-lm.html#cb30-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 108.6 on 1 and 432 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
</div>
</div>
<div id="indice-di-determinazione" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Indice di determinazione<a href="regr-model-lm.html#indice-di-determinazione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Un importante risultato dei minimi quadrati riguarda la cosiddetta <em>scomposizione della devianza</em> mediante la quale si definisce l’indice di determinazione, il quale fornisce una misura relativa della bontà di adattamento del modello di regressione ai dati del campione. Per una generica osservazione <span class="math inline">\(x_i, y_i\)</span>, la variazione di <span class="math inline">\(y_i\)</span> rispetto alla media <span class="math inline">\(\bar{y}\)</span> può essere descritta come la somma di due componenti: il residuo <span class="math inline">\(e_i=y_i- \hat{y}_i\)</span> e lo scarto di <span class="math inline">\(\hat{y}_i\)</span> rispetto alla media <span class="math inline">\(\bar{y}\)</span>:</p>
<p><span class="math display">\[
y_i - \bar{y} = (y_i- \hat{y}_i) + (\hat{y}_i - \bar{y}) = e_i + (\hat{y}_i - \bar{y}).
\]</span></p>
<p>Se consideriamo tutte le osservazioni, la devianza delle <span class="math inline">\(y\)</span> può essere scomposta nel seguente modo:</p>
<p><span class="math display">\[\begin{align}
\sum (y_i - \bar{y})^2 &amp;= \sum \left[ e_i + (\hat{y}_i - \bar{y})
\right]^2
= \sum e_i^2 + \sum (\hat{y}_i - \bar{y})^2 + 2 \sum e_i (\hat{y}_i -
\bar{y}) \notag
\end{align}\]</span></p>
<p>Per i vincoli imposti sul modello statistico di regressione, il doppio prodotto si annulla, infatti</p>
<p><span class="math display">\[\begin{align}
\sum e_i (\hat{y}_i - \bar{y}) &amp;= \sum e_i \hat{y}_i - \bar{y}\sum e_i = \sum e_i (a + b x_i) \notag \\
&amp;= a \sum e_i + b \sum e_i x_i = 0 \notag
\end{align}\]</span></p>
<p>Il termine <span class="math inline">\(b \sum e_i x_i\)</span> è uguale a zero perché, come vedremo in seguito, i coefficienti di regressione vengono calcolati in modo tale da rendere nulla <span class="math inline">\(\mbox{Cov}(e, x)\)</span>. Di conseguenza, il termine precedente deve essere nullo.</p>
<p>Possiamo dunque concludere che la devianza totale (<span class="math inline">\(\mbox{dev}_T\)</span>) si scompone nella somma di devianza d’errore (o devianza non spiegata) (<span class="math inline">\(\mbox{dev}_E\)</span>) e devianza di regressione (o devianza spiegata) (<span class="math inline">\(\mbox{dev}_T\)</span>):</p>
<p><span class="math display">\[\begin{align}
\underbrace{\sum_{i=1}^n (y_i - \bar{y})^2}_{\tiny{\text{Devianza
totale}}} &amp;= \underbrace{\sum_{i=1}^n e_i^2}_{\tiny{\text{Devianza
di dispersione}}} + \underbrace{\sum_{i=1}^n  (\hat{y}_i -
\bar{y})^2}_{\tiny{\text{Devianza di regressione}}} \notag
\end{align}\]</span></p>
<p>La devianza di regressione, <span class="math inline">\(\mbox{dev_R} \triangleq \mbox{dev_T} - \mbox{dev_E}\)</span>, indica dunque la riduzione degli errori al quadrato che è imputabile alla regressione lineare. Il rapporto <span class="math inline">\(\mbox{dev_R}/\mbox{dev_T}\)</span>, detto , esprime tale riduzione degli errori in termini proporzionali e definisce il coefficiente di correlazione al quadrato:</p>
<p><span class="math display">\[\begin{equation}
R^2 \triangleq \frac{\mbox{dev_R}}{\mbox{dev_T}} = 1 - \frac{\mbox{dev_E}}{\mbox{dev_T}}.
\end{equation}\]</span></p>
<p>Quando l’insieme di tutte le deviazioni della <span class="math inline">\(y\)</span> dalla media è spiegato dall’insieme di tutte le deviazioni della variabile teorica <span class="math inline">\(\hat{y}\)</span> dalla media, si ha che l’adattamento (o accostamento) del modello al campione di dati è perfetto, la
devianza residua è nulla ed <span class="math inline">\(r^2 = 1\)</span>; nel caso opposto, la variabilità totale coincide con quella residua, per cui <span class="math inline">\(r^2 = 0\)</span>.
Tra questi due estremi, <span class="math inline">\(r\)</span> indica l’intensità della relazione lineare tra le due variabili e <span class="math inline">\(r^2\)</span>, con <span class="math inline">\(0 \leq r^2 \leq 1\)</span>, esprime la porzione della devianza totale della <span class="math inline">\(y\)</span> che è spiegata dalla regressione lineare sulla <span class="math inline">\(x\)</span>.</p>
<p>Per l’esempio in discussione abbiamo quanto segue. La devianza totale è</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="regr-model-lm.html#cb31-1" aria-hidden="true" tabindex="-1"></a>dev_t <span class="ot">&lt;-</span> <span class="fu">sum</span>((kidiq<span class="sc">$</span>kid_score <span class="sc">-</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>kid_score))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb31-2"><a href="regr-model-lm.html#cb31-2" aria-hidden="true" tabindex="-1"></a>dev_t</span>
<span id="cb31-3"><a href="regr-model-lm.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 180386.2</span></span></code></pre></div>
<p>La devianza spiegata è</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="regr-model-lm.html#cb32-1" aria-hidden="true" tabindex="-1"></a>dev_r <span class="ot">&lt;-</span> <span class="fu">sum</span>((fm<span class="sc">$</span>fitted.values <span class="sc">-</span> <span class="fu">mean</span>(kidiq<span class="sc">$</span>kid_score))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb32-2"><a href="regr-model-lm.html#cb32-2" aria-hidden="true" tabindex="-1"></a>dev_r</span>
<span id="cb32-3"><a href="regr-model-lm.html#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 36248.82</span></span></code></pre></div>
<p>L’indice di determinazione è</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="regr-model-lm.html#cb33-1" aria-hidden="true" tabindex="-1"></a>R2 <span class="ot">&lt;-</span> dev_r <span class="sc">/</span> dev_t</span>
<span id="cb33-2"><a href="regr-model-lm.html#cb33-2" aria-hidden="true" tabindex="-1"></a>R2</span>
<span id="cb33-3"><a href="regr-model-lm.html#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.2009512</span></span></code></pre></div>
<p>Nell’output di <code>lm()</code> un tale valore è chiamato <code>Multiple R-squared</code>.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="regr-model-lm.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm)</span>
<span id="cb34-2"><a href="regr-model-lm.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb34-3"><a href="regr-model-lm.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb34-4"><a href="regr-model-lm.html#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lm(formula = kid_score ~ mom_iq, data = kidiq)</span></span>
<span id="cb34-5"><a href="regr-model-lm.html#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb34-6"><a href="regr-model-lm.html#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb34-7"><a href="regr-model-lm.html#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb34-8"><a href="regr-model-lm.html#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -56.753 -12.074   2.217  11.710  47.691 </span></span>
<span id="cb34-9"><a href="regr-model-lm.html#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb34-10"><a href="regr-model-lm.html#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb34-11"><a href="regr-model-lm.html#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb34-12"><a href="regr-model-lm.html#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; (Intercept) 25.79978    5.91741    4.36 1.63e-05 ***</span></span>
<span id="cb34-13"><a href="regr-model-lm.html#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; mom_iq       0.60997    0.05852   10.42  &lt; 2e-16 ***</span></span>
<span id="cb34-14"><a href="regr-model-lm.html#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb34-15"><a href="regr-model-lm.html#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb34-16"><a href="regr-model-lm.html#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb34-17"><a href="regr-model-lm.html#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 18.27 on 432 degrees of freedom</span></span>
<span id="cb34-18"><a href="regr-model-lm.html#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.201,  Adjusted R-squared:  0.1991 </span></span>
<span id="cb34-19"><a href="regr-model-lm.html#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; F-statistic: 108.6 on 1 and 432 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Il risultato ottenuto si può interpretare dicendo che circa il 20% della variabilità dei punteggi del QI dei bambini può essere predetto conoscendo il QI delle madri.</p>
<div id="inferenza-sul-modello-di-regressione" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Inferenza sul modello di regressione<a href="regr-model-lm.html#inferenza-sul-modello-di-regressione" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La discussione precedente era tutta basata sulla trattazione “classica” del modello lineare, ovvero una trattazione basata sulle stime di massima verosimiglianza (se <span class="math inline">\(y \sim \mathcal{N}(\alpha + \beta x, \sigma)\)</span>, allora le stime dei minimi quadrati coincidono con le stime di massima verosimiglianza). In altre parole, nella discussione precedente non abbiamo considerato in alcun modo le distribuzioni a priori dei parametri <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>. In altre parole, i risultati precedenti si confermano, in un contesto bayesiano, se e solo se imponiamo sui parametri delle distribuzioni a priori non informative (cioè, uniformi). In tali circostanze, le stime di massima verosimiglianza sono identiche al massimo a posteriori bayesiano.</p>
<p>Detto questo, il tema dell’inferenza viene trattato dall’approccio frequentista costruendo la “distribuzione campionaria” dei parametri (ovvero la distribuzione dei valori che i parametri otterrebbero in infiniti campioni casuali (<span class="math inline">\(x, y\)</span>) di ampiezza <span class="math inline">\(n\)</span> estratti dalla medesima popolazione) e poi calcolando gli errori standard dei parametri e gli intervalli di fiducia dei parametri. Una domanda frequente è, per esempio, se la pendenza della retta di regressione sia maggiore di zero. Per rispondere a tale domanda l’approccio frequentista calcola l’intervallo di fiducia al 95% per il parametro <span class="math inline">\(\beta\)</span>. Se tale intervallo non include lo zero, e se il limite inferiore di tale intervallo è maggiore di zero, allora si conclude, con un grado di confidenza del 95%, che il vero parametro <span class="math inline">\(\beta\)</span> nella popolazione è maggiore di zero. Ovvero, si conclude che vi sono evidenze di un’associazione lineare positiva tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>.</p>
<p>Alla stessa conclusione si può arrivare calcolando, in un ottica bayesiana, l’intervallo di credibilità al 95% per il parametro <span class="math inline">\(\beta\)</span>. I due intervalli sono identici se usiamo una distribuzione a priori piatta. Sono invece diversi se usiamo una distribuzione a priori debolmente informativa, oppure informativa.</p>
<p>Solitamente si usa una distribuzione a priori debolmente informativa centrata sullo zero. In tali circostanze, l’uso della distribuzione a priori ha solo un effetto di <em>regolarizzazione</em>, ovvero di riduzione del peso delle osservazioni estreme – un tale risultato statistico è molto desiderabile, ma è difficile da ottenere in un contesto frequentista. Vedremo nel prossimo capitolo come può essere svolta l’inferenza sui coefficienti del modello di regressione lineare in un contesto bayesiano.</p>
</div>
</div>
<div id="commenti-e-considerazioni-finali-1" class="section level2 unnumbered hasAnchor">
<h2>Commenti e considerazioni finali<a href="regr-model-lm.html#commenti-e-considerazioni-finali-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Il modello lineare semplice viene usato per descrivere la relazione tra due variabili e per determinare il segno e l’intensità di tale relazione. Inoltre, il modello lineare ci consente di prevedere il valore della variabile dipendente in base al valore assunto dalla variabile indipendente.</p>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge University Press.
</div>
<div class="csl-entry">
Horn, Samantha, and George Loewenstein. 2021. <span>“Underestimating Learning by Doing.”</span> <em>Available at SSRN 3941441</em>.
</div>
</div>
</div>
</div>







            </section>

          </div>
        </div>
      </div>
<a href="regr-models-intro.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/052_reglin2.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ds4psy.pdf", "ds4psy.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
