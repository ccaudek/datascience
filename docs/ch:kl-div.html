<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capitolo 2 La divergenza di Kullback-Leibler | Data Science per psicologi</title>
  <meta name="description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  <meta name="generator" content="bookdown 0.26.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Capitolo 2 La divergenza di Kullback-Leibler | Data Science per psicologi" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capitolo 2 La divergenza di Kullback-Leibler | Data Science per psicologi" />
  
  <meta name="twitter:description" content="This document contains the materials of the lessons of Psicometria B000286 (2021/2022) aimed at students of the first year of the Degree Course in Psychological Sciences and Techniques of the University of Florence, Italy." />
  

<meta name="author" content="Corrado Caudek" />


<meta name="date" content="2022-04-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch:entropy.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data science per psicologi</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefazione</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#la-psicologia-e-la-data-science"><i class="fa fa-check"></i>La psicologia e la Data science</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#come-studiare"><i class="fa fa-check"></i>Come studiare</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sviluppare-un-metodo-di-studio-efficace"><i class="fa fa-check"></i>Sviluppare un metodo di studio efficace</a></li>
</ul></li>
<li class="part"><span><b>I Il confronto bayesiano di modelli</b></span></li>
<li class="chapter" data-level="1" data-path="ch:entropy.html"><a href="ch:entropy.html"><i class="fa fa-check"></i><b>1</b> Entropia</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch:entropy.html"><a href="ch:entropy.html#la-generalizzabilità-dei-modelli"><i class="fa fa-check"></i><b>1.1</b> La generalizzabilità dei modelli</a></li>
<li class="chapter" data-level="1.2" data-path="ch:entropy.html"><a href="ch:entropy.html#capacità-predittiva"><i class="fa fa-check"></i><b>1.2</b> Capacità predittiva</a></li>
<li class="chapter" data-level="1.3" data-path="ch:entropy.html"><a href="ch:entropy.html#il-rasoio-di-ockham"><i class="fa fa-check"></i><b>1.3</b> Il rasoio di Ockham</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ch:entropy.html"><a href="ch:entropy.html#sovra-adattamento-e-sotto-adattamento"><i class="fa fa-check"></i><b>1.3.1</b> Sovra-adattamento e sotto-adattamento</a></li>
<li class="chapter" data-level="1.3.2" data-path="ch:entropy.html"><a href="ch:entropy.html#stargazing"><i class="fa fa-check"></i><b>1.3.2</b> Stargazing</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ch:entropy.html"><a href="ch:entropy.html#la-misura-del-disordine"><i class="fa fa-check"></i><b>1.4</b> La misura del disordine</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="ch:entropy.html"><a href="ch:entropy.html#entropia-di-un-singolo-evento"><i class="fa fa-check"></i><b>1.4.1</b> Entropia di un singolo evento</a></li>
<li class="chapter" data-level="1.4.2" data-path="ch:entropy.html"><a href="ch:entropy.html#entropia-di-una-variabile-casuale"><i class="fa fa-check"></i><b>1.4.2</b> Entropia di una variabile casuale</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch:entropy.html"><a href="ch:entropy.html#commenti-e-considerazioni-finali"><i class="fa fa-check"></i>Commenti e considerazioni finali</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch:kl-div.html"><a href="ch:kl-div.html"><i class="fa fa-check"></i><b>2</b> La divergenza di Kullback-Leibler</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch:kl-div.html"><a href="ch:kl-div.html#la-perdita-di-informazione"><i class="fa fa-check"></i><b>2.1</b> La perdita di informazione</a></li>
<li class="chapter" data-level="2.2" data-path="ch:kl-div.html"><a href="ch:kl-div.html#la-divergenza-dipende-dalla-direzione"><i class="fa fa-check"></i><b>2.2</b> La divergenza dipende dalla direzione</a></li>
<li class="chapter" data-level="2.3" data-path="ch:kl-div.html"><a href="ch:kl-div.html#confronto-tra-modelli"><i class="fa fa-check"></i><b>2.3</b> Confronto tra modelli</a></li>
<li class="chapter" data-level="2.4" data-path="ch:kl-div.html"><a href="ch:kl-div.html#expected-log-predictive-density"><i class="fa fa-check"></i><b>2.4</b> Expected log predictive density</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ch:kl-div.html"><a href="ch:kl-div.html#log-pointwise-predictive-density"><i class="fa fa-check"></i><b>2.4.1</b> Log pointwise predictive density</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ch:kl-div.html"><a href="ch:kl-div.html#commenti-e-considerazioni-finali-1"><i class="fa fa-check"></i>Commenti e considerazioni finali</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science per psicologi</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch:kl-div" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Capitolo 2</span> La divergenza di Kullback-Leibler<a href="ch:kl-div.html#ch:kl-div" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>È comune in statistica utilizzare una distribuzione di probabilità <span class="math inline">\(q\)</span> per approssimare un’altra distribuzione <span class="math inline">\(p\)</span> – generalmente, questo viene fatto se <span class="math inline">\(p\)</span> non è conosciuta o è troppo complessa. In questi casi possiamo chiederci quanta informazione venga perduta usando <span class="math inline">\(q\)</span> al posto di <span class="math inline">\(p\)</span>, o equivalentemente, quanta incertezza aggiuntiva viene introdotta nell’analisi statistica. La quantificazione di questo incremento di incertezza è fornita dalla divergenza di Kullback-Leibler.</p>
<div id="la-perdita-di-informazione" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> La perdita di informazione<a href="ch:kl-div.html#la-perdita-di-informazione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Intuitivamente, per quantificare l’informazione che si perde quando una distribuzione approssimata <span class="math inline">\(q\)</span> viene usata in luogo della distribuzione corretta <span class="math inline">\(p\)</span> sembra necessaria una quantità che ha valore zero quando <span class="math inline">\(q = p\)</span>, e un valore positivo altrimenti. Seguendo la definizione <a href="ch:entropy.html#eq:entropy">(1.1)</a> di entropia, possiamo quantificare una tale perdita di informazione mediante il valore atteso della differenza tra <span class="math inline">\(\log(p)\)</span> e <span class="math inline">\(\log(q)\)</span>. Questa quantità è chiamata <em>entropia relativa</em> o <em>divergenza di Kullback-Leibler</em>:</p>
<p><span class="math display" id="eq:kldivergence">\[\begin{equation}
\mathbb{KL} (p \mid\mid q) = \E (\log p - \log q).
\tag{2.1}
\end{equation}\]</span></p>
<p>La divergenza <span class="math inline">\(\mathbb{KL} (p \mid\mid q)\)</span> corrisponde alla differenza media nelle probabilità logaritmiche quando <span class="math inline">\(q\)</span> viene usato per approssimare <span class="math inline">\(p\)</span>. Poiché gli eventi si manifestano secondo <span class="math inline">\(p\)</span>, è necessario calcolare il valore atteso rispetto a <span class="math inline">\(p\)</span>. Per distribuzioni discrete dunque abbiamo:</p>
<p><span class="math display">\[\begin{equation}
\mathbb{KL} (p \mid\mid q) = \sum_i^n p_i (\log p_i - \log q_i) = \sum_i^n p_i \log \frac{p_i}{q_i}.
\end{equation}\]</span></p>
<p>Riarrangiando i termini otteniamo:</p>
<p><span class="math display">\[\begin{equation}
\mathbb{KL} (p \mid\mid q) = -\sum_i^n p_i (\log q_i - \log p_i),
\end{equation}\]</span></p>
<p>ovvero,</p>
<p><span class="math display">\[\begin{equation}
\mathbb{KL} (p \mid\mid q) = \underbrace{-\sum_i^n p_i \log q_i}_{h(p, q)} - \underbrace{\left(-\sum_i^n p_i \log p_i\right)}_{h(p)},
\end{equation}\]</span></p>
<p>laddove <span class="math inline">\(h(p)\)</span> è l’entropia di <span class="math inline">\(p\)</span> e <span class="math inline">\(h(p, q) = −\E [\log q]\)</span> può essere intesa come l’entropia di <span class="math inline">\(q\)</span>, ma valutata secondo i valori di probabilità <span class="math inline">\(p\)</span>.</p>
<p>Riarrangiando l’equazione precedente otteniamo:</p>
<p><span class="math display">\[\begin{equation}
h(p, q) = h(p) + \mathbb{KL} (p \mid\mid q),
\end{equation}\]</span></p>
<p>il che mostra come la divergenza <span class="math inline">\(\mathbb{KL}\)</span> possa essere interpretata come l’incremento di entropia, rispetto a <span class="math inline">\(h(p)\)</span>, quando <span class="math inline">\(q\)</span> viene usata per rappresentare <span class="math inline">\(p\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Esempio 2.1  </strong></span><span class="citation">(da <a href="#ref-McElreath_rethinking" role="doc-biblioref">McElreath 2020</a>)</span> Sia la distribuzione target <span class="math inline">\(p = \{0.3, 0.7\}\)</span>. Supponiamo che la distribuzione approssimata <span class="math inline">\(q\)</span> possa assumere valori da <span class="math inline">\(q = \{0.01, 0.99\}\)</span> a <span class="math inline">\(q = \{0.99, 0.01\}\)</span>. Calcoliamo la divergenza KL.</p>
<p>Le istruzioni <span class="math inline">\(\R\)</span> sono le seguenti:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="ch:kl-div.html#cb7-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span></span>
<span id="cb7-2"><a href="ch:kl-div.html#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb7-3"><a href="ch:kl-div.html#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">p_1 =</span> .<span class="dv">3</span>,</span>
<span id="cb7-4"><a href="ch:kl-div.html#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">p_2 =</span> .<span class="dv">7</span>,</span>
<span id="cb7-5"><a href="ch:kl-div.html#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">q_1 =</span> <span class="fu">seq</span>(<span class="at">from =</span> .<span class="dv">01</span>, <span class="at">to =</span> .<span class="dv">99</span>, <span class="at">by =</span> .<span class="dv">01</span>)</span>
<span id="cb7-6"><a href="ch:kl-div.html#cb7-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb7-7"><a href="ch:kl-div.html#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb7-8"><a href="ch:kl-div.html#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">q_2 =</span> <span class="dv">1</span> <span class="sc">-</span> q_1</span>
<span id="cb7-9"><a href="ch:kl-div.html#cb7-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb7-10"><a href="ch:kl-div.html#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb7-11"><a href="ch:kl-div.html#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">d_kl =</span> (p_1 <span class="sc">*</span> <span class="fu">log</span>(p_1 <span class="sc">/</span> q_1)) <span class="sc">+</span> (p_2 <span class="sc">*</span> <span class="fu">log</span>(p_2 <span class="sc">/</span> q_2))</span>
<span id="cb7-12"><a href="ch:kl-div.html#cb7-12" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb7-13"><a href="ch:kl-div.html#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="ch:kl-div.html#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(t)</span>
<span id="cb7-15"><a href="ch:kl-div.html#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; # A tibble: 6 × 5</span></span>
<span id="cb7-16"><a href="ch:kl-div.html#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     p_1   p_2   q_1   q_2  d_kl</span></span>
<span id="cb7-17"><a href="ch:kl-div.html#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb7-18"><a href="ch:kl-div.html#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1   0.3   0.7  0.01  0.99 0.778</span></span>
<span id="cb7-19"><a href="ch:kl-div.html#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2   0.3   0.7  0.02  0.98 0.577</span></span>
<span id="cb7-20"><a href="ch:kl-div.html#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3   0.3   0.7  0.03  0.97 0.462</span></span>
<span id="cb7-21"><a href="ch:kl-div.html#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4   0.3   0.7  0.04  0.96 0.383</span></span>
<span id="cb7-22"><a href="ch:kl-div.html#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5   0.3   0.7  0.05  0.95 0.324</span></span>
<span id="cb7-23"><a href="ch:kl-div.html#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6   0.3   0.7  0.06  0.94 0.276</span></span></code></pre></div>
<p>
Nella figura seguente sull’asse delle ascisse sono rappresentati i valori <span class="math inline">\(q\)</span> e sull’asse delle ordinante sono riportati i corrispondenti valori <span class="math inline">\(\mathbb{KL}\)</span>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="ch:kl-div.html#cb8-1" aria-hidden="true" tabindex="-1"></a>t <span class="sc">%&gt;%</span></span>
<span id="cb8-2"><a href="ch:kl-div.html#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> q_1, <span class="at">y =</span> d_kl)) <span class="sc">+</span></span>
<span id="cb8-3"><a href="ch:kl-div.html#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> .<span class="dv">3</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb8-4"><a href="ch:kl-div.html#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb8-5"><a href="ch:kl-div.html#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="at">geom =</span> <span class="st">&quot;text&quot;</span>, <span class="at">x =</span> .<span class="dv">4</span>, <span class="at">y =</span> <span class="fl">1.5</span>, <span class="at">label =</span> <span class="st">&quot;q = p&quot;</span>,</span>
<span id="cb8-6"><a href="ch:kl-div.html#cb8-6" aria-hidden="true" tabindex="-1"></a>           <span class="at">size =</span> <span class="fl">3.5</span>) <span class="sc">+</span></span>
<span id="cb8-7"><a href="ch:kl-div.html#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;q[1]&quot;</span>,</span>
<span id="cb8-8"><a href="ch:kl-div.html#cb8-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Divergenza di q da p&quot;</span>)</span></code></pre></div>
<p><img src="ds4psy_files/figure-html/unnamed-chunk-11-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Tanto meglio la distribuzione <span class="math inline">\(q\)</span> approssima la distribuzione target tanto più piccolo è il valore di divergenza KL.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Esempio 2.2  </strong></span>Sia <span class="math inline">\(p\)</span> una distribuzione binomiale di parametri <span class="math inline">\(\theta = 0.2\)</span> e <span class="math inline">\(n = 5\)</span></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="ch:kl-div.html#cb9-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb9-2"><a href="ch:kl-div.html#cb9-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb9-3"><a href="ch:kl-div.html#cb9-3" aria-hidden="true" tabindex="-1"></a>true_py <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span>n, n, <span class="fl">0.2</span>)</span>
<span id="cb9-4"><a href="ch:kl-div.html#cb9-4" aria-hidden="true" tabindex="-1"></a>true_py</span>
<span id="cb9-5"><a href="ch:kl-div.html#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.4096 0.4096 0.1536 0.0256 0.0016</span></span></code></pre></div>
<p>
Sia <span class="math inline">\(q_1\)</span> una approssimazione a <span class="math inline">\(p\)</span>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="ch:kl-div.html#cb10-1" aria-hidden="true" tabindex="-1"></a>q1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.46</span>, <span class="fl">0.42</span>, <span class="fl">0.10</span>, <span class="fl">0.01</span>, <span class="fl">0.01</span>)</span>
<span id="cb10-2"><a href="ch:kl-div.html#cb10-2" aria-hidden="true" tabindex="-1"></a>q1</span>
<span id="cb10-3"><a href="ch:kl-div.html#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.46 0.42 0.10 0.01 0.01</span></span></code></pre></div>
<p>Sia <span class="math inline">\(q_2\)</span> una distribuzione uniforme:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="ch:kl-div.html#cb11-1" aria-hidden="true" tabindex="-1"></a>q2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fl">0.2</span>, <span class="dv">5</span>)</span>
<span id="cb11-2"><a href="ch:kl-div.html#cb11-2" aria-hidden="true" tabindex="-1"></a>q2</span>
<span id="cb11-3"><a href="ch:kl-div.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.2 0.2 0.2 0.2 0.2</span></span></code></pre></div>
<p>La divergenza KL di <span class="math inline">\(q_1\)</span> da <span class="math inline">\(p\)</span> è</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="ch:kl-div.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(true_py <span class="sc">*</span> <span class="fu">log</span>(true_py <span class="sc">/</span> q1))</span>
<span id="cb12-2"><a href="ch:kl-div.html#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.02925199</span></span></code></pre></div>
<p>La divergenza KL di <span class="math inline">\(q_2\)</span> da <span class="math inline">\(p\)</span> è:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="ch:kl-div.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(true_py <span class="sc">*</span> <span class="fu">log</span>(true_py <span class="sc">/</span> q2))</span>
<span id="cb13-2"><a href="ch:kl-div.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.4863578</span></span></code></pre></div>
<p>È chiaro che perdiamo una quantità maggiore di informazioni se, per descrivere la distribuzione binomiale <span class="math inline">\(p\)</span>, usiamo la distribuzione uniforme <span class="math inline">\(q_2\)</span> anziché <span class="math inline">\(q_1\)</span>.</p>
</div>
</div>
<div id="la-divergenza-dipende-dalla-direzione" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> La divergenza dipende dalla direzione<a href="ch:kl-div.html#la-divergenza-dipende-dalla-direzione" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La divergenza KL non è una vera e propria metrica: per esempio, non è simmetrica. In generale, <span class="math inline">\(\mathbb{KL}(p \mid\mid q) \neq \mathbb{KL}(q \mid\mid p)\)</span>, ovvero la <span class="math inline">\(\mathbb{KL}\)</span> da <span class="math inline">\(p\)</span> a <span class="math inline">\(q\)</span> è diversa dalla <span class="math inline">\(\mathbb{KL}\)</span> da <span class="math inline">\(q\)</span> a <span class="math inline">\(p\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Esempio 2.3  </strong></span>Usando le seguenti istruzioni <span class="math inline">\(\R\)</span> otteniamo:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="ch:kl-div.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">direction =</span> <span class="fu">c</span>(<span class="st">&quot;Da q a p&quot;</span>, <span class="st">&quot;Da p a q&quot;</span>),</span>
<span id="cb14-2"><a href="ch:kl-div.html#cb14-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">p_1 =</span> <span class="fu">c</span>(.<span class="dv">01</span>, .<span class="dv">7</span>),</span>
<span id="cb14-3"><a href="ch:kl-div.html#cb14-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">q_1 =</span> <span class="fu">c</span>(.<span class="dv">7</span>, .<span class="dv">01</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb14-4"><a href="ch:kl-div.html#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p_2 =</span> <span class="dv">1</span> <span class="sc">-</span> p_1,</span>
<span id="cb14-5"><a href="ch:kl-div.html#cb14-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">q_2 =</span> <span class="dv">1</span> <span class="sc">-</span> q_1) <span class="sc">%&gt;%</span></span>
<span id="cb14-6"><a href="ch:kl-div.html#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">d_kl =</span> (p_1 <span class="sc">*</span> <span class="fu">log</span>(p_1 <span class="sc">/</span> q_1)) <span class="sc">+</span> (p_2 <span class="sc">*</span> <span class="fu">log</span>(p_2 <span class="sc">/</span> q_2)))</span>
<span id="cb14-7"><a href="ch:kl-div.html#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; # A tibble: 2 × 6</span></span>
<span id="cb14-8"><a href="ch:kl-div.html#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   direction   p_1   q_1   p_2   q_2  d_kl</span></span>
<span id="cb14-9"><a href="ch:kl-div.html#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span></span>
<span id="cb14-10"><a href="ch:kl-div.html#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 Da q a p   0.01  0.7   0.99  0.3   1.14</span></span>
<span id="cb14-11"><a href="ch:kl-div.html#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 Da p a q   0.7   0.01  0.3   0.99  2.62</span></span></code></pre></div>
</div>
</div>
<div id="confronto-tra-modelli" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Confronto tra modelli<a href="ch:kl-div.html#confronto-tra-modelli" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La divergenza KL viene utilizzata nel confronto tra modelli, ovvero ci consente di quantificare l’informazione che viene perduta quando utilizziamo la distribuzione di probabilità ipotizzata da un modello, chiamiamola <span class="math inline">\(p_{\mathcal{M}}\)</span>, per approssimare la distribuzione di probabilità del vero modello generatore dei dati, <span class="math inline">\(p_t\)</span>.</p>
<p>Nel Capitolo <a href="#ch:intro-bayes-inference"><strong>??</strong></a> abbiamo introdotto il concetto di distribuzione predittiva a posteriori:</p>
<p><span class="math display">\[
p(\tilde{y} \mid y) = \int_\Theta p(\tilde{y} \mid \theta) p(\theta \mid y) \,\operatorname {d}\!\theta .
\]</span></p>
<p>La distribuzione predittiva a posteriori descrive il tipo di dati che ci aspettiamo vengano prodotti dal modello generativo <span class="math inline">\(\mathcal{M}\)</span>, alla luce delle nostre credenze iniziali, <span class="math inline">\(p(\theta)\)</span> e dei dati osservati <span class="math inline">\(y\)</span>. Quando valutiamo un modello ci chiediamo in che misura <span class="math inline">\(p_{\mathcal{M}}(\tilde{y} \mid y)\)</span> approssimi <span class="math inline">\(p_t(\tilde{y})\)</span>. Cioè, ci chiediamo quanto siano simili i dati <span class="math inline">\(p_{\mathcal{M}}(\cdot)\)</span> prodotti dal modello <span class="math inline">\(\mathcal{M}\)</span> ai dati prodotti dal vero processo generatore dei dati <span class="math inline">\(p_t(\cdot)\)</span>.</p>
<p>Una misura della “somiglianza” tra la distribuzione <span class="math inline">\(q_{\mathcal{M}}\)</span> ipotizzata dal modello <span class="math inline">\(\mathcal{M}\)</span> e la distribuzione <span class="math inline">\(p_t\)</span> del vero modello generatore dei dati è fornita dalla divergenza di Kullback-Leibler <span class="math inline">\(\mathbb{KL}(p_t \mid\mid q_{\mathcal{M}})\)</span>. Supponendo di avere <span class="math inline">\(k\)</span> modelli della distribuzione a posteriori, <span class="math inline">\(\{q_{\mathcal{M}_1}, q_{\mathcal{M}_2}, \dots, q_{\mathcal{M}_k}\}\)</span>, e di conoscere il vero modello generatore dei dati, possiamo scrivere</p>
<p><span class="math display" id="eq:kl-mod-comp">\[\begin{align}
\mathbb{KL} (p_t \mid\mid q_{\mathcal{M}_1}) &amp;= \E (\log p_{\mathcal{M}_0}) - \E (\log q_{\mathcal{M}_1})\notag\\
\mathbb{KL} (p_t \mid\mid q_{\mathcal{M}_2}) &amp;= \E (\log p_t) - \E (\log q_{\mathcal{M}_2})\notag\\
&amp;\cdots\notag\\
\mathbb{KL} (p_t \mid\mid q_{\mathcal{M}_k}) &amp;= \E (\log p_{\mathcal{M}_0}) - \E (\log q_{\mathcal{M}_k}).
\tag{2.2}
\end{align}\]</span></p>
<p>La <a href="ch:kl-div.html#eq:kl-mod-comp">(2.2)</a> può sembrare un esercizio futile poiché nella vita reale non conosciamo il vero modello generatore dei dati. È però facile rendersi conto che, poiché <span class="math inline">\(p_t\)</span> è la stessa per tutti i confronti, diventa possibile costruire un ordinamento dei modelli basato unicamente sul secondo termine della <a href="ch:kl-div.html#eq:kl-mod-comp">(2.2)</a>, ovvero senza nessun riferimento al vero modello generatore dei dati. Per un generico modello <span class="math inline">\(\mathcal{M}\)</span>, il secondo termine della <a href="ch:kl-div.html#eq:kl-mod-comp">(2.2)</a> può essere scritto come:</p>
<p><span class="math display" id="eq:kl-div-cont-t2">\[\begin{equation}
\E \log p_{\mathcal{M}}(y) = \int_{-\infty}^{+\infty}p_{t}(y)\log p_{\mathcal{M}}(y) \,\operatorname {d}\!y .
\tag{2.3}
\end{equation}\]</span></p>
</div>
<div id="expected-log-predictive-density" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Expected log predictive density<a href="ch:kl-div.html#expected-log-predictive-density" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Le previsioni del modello <span class="math inline">\(\mathcal{M}\)</span> sui nuovi dati futuri sono date dalla distribuzione predittiva a posteriori. Possiamo dunque riscrivere la <a href="ch:kl-div.html#eq:kl-div-cont-t2">(2.3)</a> come</p>
<p><span class="math display" id="eq:elpd">\[\begin{equation}
\elpd = \int_{\tilde{y}} p_{t}(\tilde{y}) \log p(\tilde{y} \mid y) \,\operatorname {d}\!\tilde{y}.
\tag{2.4}
\end{equation}\]</span></p>
<p>La <a href="ch:kl-div.html#eq:elpd">(2.4)</a> è chiamata <em>expected log predictive density</em> (<span class="math inline">\(\elpd\)</span>) e fornisce la risposta al problema che ci eravamo posti: nel confronto tra modelli, come è possibile scegliere il modello più simile al vero meccanismo generatore dei dati? Possiamo pensare alla <a href="ch:kl-div.html#eq:elpd">(2.4)</a> dicendo che descrive la distribuzione predittiva a posteriori del modello ponderando la verosimiglianza dei possibili (sconosciuti) dati futuri (<span class="math inline">\(\tilde{y}\)</span>) con la vera distribuzione <span class="math inline">\(p_t\)</span>. Di conseguenza, valori <span class="math inline">\(\elpd\)</span> più grandi identificano il modello che risulta più simile al vero meccanismo generatore dei dati.</p>
<p>Non dobbiamo preoccuparci di trovare una formulazione analitica della distribuzione predittiva a posteriori <span class="math inline">\(p(\tilde{y} \mid y)\)</span> perché, come abbiamo visto nel Capitolo <a href="#chapter-ppc"><strong>??</strong></a>, è possibile approssimare tale distribuzione mediante simulazione. Notiamo però che la <a href="ch:kl-div.html#eq:elpd">(2.4)</a> include un termine, <span class="math inline">\(p_t(\tilde{y})\)</span>, il quale descrive la distribuzione dei dati futuri <span class="math inline">\(\tilde{y}\)</span> secondo il vero modello generatore dei dati. Il termine <span class="math inline">\(p_t\)</span>, ovviamente, è ignoto.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> Di conseguenza, la quantità <span class="math inline">\(\elpd\)</span> non può mai essere calcolata in maniera esatta, ma può solo essere stimata. Il secondo problema di questo Capitolo è capire come la <a href="ch:kl-div.html#eq:elpd">(2.4)</a> possa essere stimata utilizzando un campione di osservazioni.</p>
<div id="log-pointwise-predictive-density" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Log pointwise predictive density<a href="ch:kl-div.html#log-pointwise-predictive-density" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ingenuamente, potremmo pensare di stimare la <a href="ch:kl-div.html#eq:elpd">(2.4)</a> ipotizzando che la distribuzione del campione coincida con <span class="math inline">\(p_t\)</span>. Usare la distribuzione del campione come proxy del vero modello generatore dei dati (ovvero, ipotizzare che la distribuzione del campione rappresenti fedelmente <span class="math inline">\(p_t\)</span>) comporta due conseguenze:</p>
<ul>
<li>non è necessario ponderare per <span class="math inline">\(p_t\)</span>, in quanto assumiamo che la distribuzione empirica del campione corrisponda a <span class="math inline">\(p_t\)</span> (ciò significa assumere che i valori più comunemente osservati nel campione siano anche quelli più verosimili nella vera distribuzione <span class="math inline">\(p_t\)</span>);</li>
<li>dato che il campione è finito, anziché eseguire un’operazione di integrazione possiamo semplicemente sommare la densità predittiva a posteriori delle osservazioni.</li>
</ul>
<p>Questo conduce alla seguente equazione:<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p><span class="math display" id="eq:1n-lppd">\[\begin{equation}
\frac{1}{n} \sum_{i=1}^n \log p(y_i^{rep} \mid y).
\tag{2.5}
\end{equation}\]</span></p>
<p>La quantità <a href="ch:kl-div.html#eq:1n-lppd">(2.5)</a>, senza il passaggio finale della divisione per il numero di osservazioni, è chiamata <em>log pointwise predictive density</em> (<span class="math inline">\(\lppd\)</span>)</p>
<p><span class="math display" id="eq:lppd">\[\begin{equation}
\lppd = \sum_{i=1}^n \log p(y_i^{rep} \mid y)
\tag{2.6}
\end{equation}\]</span></p>
<p>e corrisponde alla somma delle densità predittive logaritmiche delle <span class="math inline">\(n\)</span> osservazioni. Valori più grandi della <a href="ch:kl-div.html#eq:lppd">(2.6)</a> sono da preferire perché indicano una maggiore accuratezza media. È anche comune vedere espressa la quantità precedente nei termini della <em>devianza</em>, ovvero alla <span class="math inline">\(\lppd\)</span> moltiplicata per -2. In questo secondo caso sono da preferire valori piccoli.</p>
<p>È importante notare che <span class="math inline">\(\lppd\)</span> fornisce una <em>sovrastima</em> della <a href="ch:kl-div.html#eq:elpd">(2.4)</a>. Tale sovrastima è dovuta al fatto che, nel calcolo della <a href="ch:kl-div.html#eq:lppd">(2.6)</a>, abbiamo usato <span class="math inline">\(p(y^{rep} \mid y)\)</span> al posto di <span class="math inline">\(p(\tilde{y} \mid y)\)</span>: in altri termini, abbiamo considerato le osservazioni del campione come se fossero un nuovo campione di dati. In una serie di simulazioni, <span class="citation">McElreath (<a href="#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span> esamina il significato di questa sovrastima. Nelle simulazioni la devianza viene calcolata come funzione della complessità (ovvero, il numero di parametri) del modello. La simulazione mostra che <span class="math inline">\(\lppd\)</span> aumenta al crescere del numero di parametri del modello. Ciò significa che <span class="math inline">\(\lppd\)</span> mostra lo stesso limite del coefficiente di determinazione: aumenta all’aumentare della complessità del modello.</p>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Esempio 2.4  </strong></span>Esaminiamo un esempio tratto da <a href="https://vasishth.github.io/bayescogsci/book/expected-log-predictive-density-of-a-model.html">Bayesian Data Analysis for Cognitive Science</a> nel quale la <span class="math inline">\(\elpd\)</span> viene calcolata in forma esatta oppure mediante approssimazione. Supponiamo di disporre di un campione di <span class="math inline">\(n\)</span> osservazioni. Supponiamo inoltre di conoscere il vero processo generativo dei dati (qualcosa che in pratica non è mai possibile), ovvero:</p>
<p><span class="math display">\[
p_t(y) = \Beta(1, 3).
\]</span>
I dati sono</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="ch:kl-div.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">75</span>)</span>
<span id="cb15-2"><a href="ch:kl-div.html#cb15-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb15-3"><a href="ch:kl-div.html#cb15-3" aria-hidden="true" tabindex="-1"></a>y_data <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(n, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb15-4"><a href="ch:kl-div.html#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(y_data)</span>
<span id="cb15-5"><a href="ch:kl-div.html#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.55062422 0.13346270 0.80250987 0.21430898 0.01913430 0.08676517</span></span></code></pre></div>
<p>Supponiamo inoltre di avere adattato ai dati un modello bayesiano <span class="math inline">\(\mathcal{M}\)</span> e di avere ottenuto la distribuzione a posteriori per i parametri del modello. Inoltre, supponiamo di avere derivato la forma analitica della distribuzione predittiva a posteriori per il modello:</p>
<p><span class="math display">\[
p(y^{rep} \mid y) \sim \Beta(2, 2).
\]</span></p>
<p>Questa distribuzione ci dice quanto sono credibili i possibili dati futuri.</p>
<p>Conoscendo la vera distribuzione dei dati <span class="math inline">\(p_t(y)\)</span> possiamo calcolare in forma esatta la quantità <span class="math inline">\(\elpd\)</span>, ovvero</p>
<p><span class="math display">\[
\elpd = \int_{y^{rep}}p_{t}(y^{rep})\log p(y^{rep} \mid y) \,\operatorname {d}\!y^{rep}.
\]</span></p>
<p>Svolgiamo i calcoli in <span class="math inline">\(\R\)</span> otteniamo:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="ch:kl-div.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># True distribution</span></span>
<span id="cb16-2"><a href="ch:kl-div.html#cb16-2" aria-hidden="true" tabindex="-1"></a>p_t <span class="ot">&lt;-</span> <span class="cf">function</span>(y) <span class="fu">dbeta</span>(y, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb16-3"><a href="ch:kl-div.html#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictive distribution</span></span>
<span id="cb16-4"><a href="ch:kl-div.html#cb16-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="cf">function</span>(y) <span class="fu">dbeta</span>(y, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb16-5"><a href="ch:kl-div.html#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Integration</span></span>
<span id="cb16-6"><a href="ch:kl-div.html#cb16-6" aria-hidden="true" tabindex="-1"></a>integrand <span class="ot">&lt;-</span> <span class="cf">function</span>(y) <span class="fu">p_t</span>(y) <span class="sc">*</span> <span class="fu">log</span>(<span class="fu">p</span>(y))</span>
<span id="cb16-7"><a href="ch:kl-div.html#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="at">f =</span> integrand, <span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>)</span>
<span id="cb16-8"><a href="ch:kl-div.html#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -0.3749072 with absolute error &lt; 6.8e-07</span></span></code></pre></div>
<p>Tuttavia, in pratica non conosciamo mai <span class="math inline">\(p_t(y)\)</span>. Quindi approssimiamo <span class="math inline">\(\elpd\)</span> usando la <a href="ch:kl-div.html#eq:elpd">(2.4)</a>:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^n \log p(y_i \mid y).
\]</span></p>
<p>Così facendo, e svolgendo i calcoli in <span class="math inline">\(\R\)</span>, otteniamo un valore diverso da quello trovato in precedenza:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="ch:kl-div.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">/</span>n <span class="sc">*</span> <span class="fu">sum</span>(<span class="fu">log</span>(<span class="fu">p</span>(y_data)))</span>
<span id="cb17-2"><a href="ch:kl-div.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] -0.3639141</span></span></code></pre></div>
</div>
</div>
</div>
<div id="commenti-e-considerazioni-finali-1" class="section level2 unnumbered hasAnchor">
<h2>Commenti e considerazioni finali<a href="ch:kl-div.html#commenti-e-considerazioni-finali-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dato che non conosciamo il vero meccanismo generatore dei dati <span class="math inline">\(p\)</span>, possiamo usare la distribuzione dei dati osservata come proxy per la vera distribuzione <span class="math inline">\(p\)</span>. Quindi, invece di ponderare la distribuzione predittiva in base alla densità reale di tutti i possibili dati futuri, utilizziamo semplicemente le <span class="math inline">\(n\)</span> osservazioni che abbiamo. Possiamo farlo perché assumiamo che le nostre osservazioni costituiscano un campione dalla vera distribuzione dei dati: in base a questa ipotesi, nel campione ci aspettiamo di osservare più frequentemente quelle osservazioni che hanno una maggiore verosimiglianza nella vera distribuzione <span class="math inline">\(p\)</span>. È così possibile giungere ad una stima numerica della <span class="math inline">\(\elpd\)</span> chiamata <em>log pointwise predictive density</em> (<span class="math inline">\(\lppd\)</span>).</p>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Gelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. <span>“Understanding Predictive Information Criteria for Bayesian Models.”</span> <em>Statistics and Computing</em> 24 (6): 997–1016.
</div>
<div class="csl-entry">
Hoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. <span>“Bayesian Model Averaging: A Tutorial (with Comments by m. Clyde, David Draper and EI George, and a Rejoinder by the Authors.”</span> <em>Statistical Science</em> 14 (4): 382–417.
</div>
<div class="csl-entry">
Horn, Samantha, and George Loewenstein. 2021. <span>“Underestimating Learning by Doing.”</span> <em>Available at SSRN 3941441</em>.
</div>
<div class="csl-entry">
Johnson, Alicia A., Miles Ott, and Mine Dogucu. 2022. <em><span class="nocase">Bayes Rules! An Introduction to Bayesian Modeling with R</span></em>. CRC Press.
</div>
<div class="csl-entry">
Lord, Frederic M. 1950. <span>“Efficiency of Prediction When a Regression Equation from One Sample Is Used in a New Sample.”</span> <em>ETS Research Bulletin Series</em> 1950 (2): 1–6.
</div>
<div class="csl-entry">
Martin, Osvaldo A, Ravin Kumar, and Junpeng Lao. 2022. <em>Bayesian Modeling and Computation in Python</em>. CRC Press.
</div>
<div class="csl-entry">
McElreath, Richard. 2020. <em>Statistical Rethinking: <span>A</span> <span>Bayesian</span> Course with Examples in <span>R</span> and <span>Stan</span></em>. 2nd Edition. Boca Raton, Florida: CRC Press.
</div>
<div class="csl-entry">
Song, Q Chelsea, Chen Tang, and Serena Wee. 2021. <span>“Making Sense of Model Generalizability: A Tutorial on Cross-Validation in r and Shiny.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 4 (1): 2515245920947067.
</div>
</div>
</div>
</div>










<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gelman2014understanding" class="csl-entry">
Gelman, Andrew, Jessica Hwang, and Aki Vehtari. 2014. <span>“Understanding Predictive Information Criteria for Bayesian Models.”</span> <em>Statistics and Computing</em> 24 (6): 997–1016.
</div>
<div id="ref-McElreath_rethinking" class="csl-entry">
McElreath, Richard. 2020. <em>Statistical Rethinking: <span>A</span> <span>Bayesian</span> Course with Examples in <span>R</span> and <span>Stan</span></em>. 2nd Edition. Boca Raton, Florida: CRC Press.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>Se il modello sottostante i dati fosse noto non avremmo bisogno di cercare il modello migliore, perché <span class="math inline">\(p_t\)</span> è il modello migliore.<a href="ch:kl-div.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>In riferimento alla notazione, ricordiamo che <span class="citation">Gelman, Hwang, and Vehtari (<a href="#ref-gelman2014understanding" role="doc-biblioref">2014</a>)</span> distinguono tra <span class="math inline">\(y^{rep}\)</span> e <span class="math inline">\(\tilde{y}\)</span>. I valori <span class="math inline">\(y^{rep}\)</span> corrispondono ad un’altra possibile realizzazione del medesimo modello statistico che ha prodotto <span class="math inline">\(y\)</span> mediante determinati valori dei parametri <span class="math inline">\(\theta\)</span> (repliche sotto lo stesso modello statistico). I valori <span class="math inline">\(\tilde{y}\)</span> corrispondono invece ad un campione empirico di dati osservato in qualche futura occasione.<a href="ch:kl-div.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch:entropy.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/091_kl.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ds4psy.pdf", "ds4psy.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
